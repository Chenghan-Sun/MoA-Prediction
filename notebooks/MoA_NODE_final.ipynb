{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_NODE_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chenghan-Sun/MoA-Prediction/blob/master/notebooks/MoA_NODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uXqumWhS_M"
      },
      "source": [
        "# Experiment of NEURAL OBLIVIOUS DECISION ENSEMBLES (NODE) on MoA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3AoDEk9UKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3063f70-db99-4492-e041-86ff8783ea45"
      },
      "source": [
        "# !git clone https://github.com/Qwicen/node.git\n",
        "# !pip install -r node/requirements.txt\n",
        "# !pip install qhoptim\n",
        "\n",
        "# only need this one\n",
        "! pip install iterative-stratification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M5Eop_KvORv"
      },
      "source": [
        "import datetime\n",
        "import gc\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import  os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "from typing import Optional\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAvLfyoE1Rh"
      },
      "source": [
        "Run the following cell if you want to use GDrive (file stored under `/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3JosjdMzA5s",
        "outputId": "de554de0-f661-41f5-88a9-bced7029faae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3SEb04vORw",
        "outputId": "71a262e7-7ee8-4516-abeb-39ec9696f088"
      },
      "source": [
        "# path in GDrive\n",
        "# data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA/lish-moa/\"\n",
        "\n",
        "# ------------------ Used when data is in the google drive --------------------#\n",
        "# path\n",
        "#!unzip \"/content/drive/Shareddrives/moa_data/lish-moa.zip\"\n",
        "#data_path = \"/content/\"\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "data_file_list = [\"train_features.csv\", \"train_targets_scored.csv\"]\n",
        "\n",
        "# load data\n",
        "data_train = pd.read_csv(data_path + data_file_list[0])\n",
        "train_targets_scored = pd.read_csv(data_path + data_file_list[1])\n",
        "\n",
        "# data info\n",
        "print(f'Training features file: {data_train.shape[0]} rows; {data_train.shape[1]} columns')\n",
        "print(f'Training targets scored file: {train_targets_scored.shape[0]} rows; \\\n",
        "{train_targets_scored.shape[1]} columns')\n",
        "\n",
        "# Train-test split\n",
        "data_train, data_test, train_targets_scored , test_targets_scored  = train_test_split(data_train, train_targets_scored, test_size = 0.1, random_state = 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features file: 23814 rows; 876 columns\n",
            "Training targets scored file: 23814 rows; 207 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "c0QKkoK_vOR1",
        "outputId": "e72288c3-15fe-45ae-d600-994a011f1e16"
      },
      "source": [
        "data_train.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3611</th>\n",
              "      <td>id_26c42f465</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D2</td>\n",
              "      <td>0.2894</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>-3.6110</td>\n",
              "      <td>-0.4329</td>\n",
              "      <td>-0.3965</td>\n",
              "      <td>0.1353</td>\n",
              "      <td>1.0590</td>\n",
              "      <td>-0.0249</td>\n",
              "      <td>2.4480</td>\n",
              "      <td>0.1467</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>-0.1511</td>\n",
              "      <td>1.4710</td>\n",
              "      <td>0.8354</td>\n",
              "      <td>0.0756</td>\n",
              "      <td>-0.2370</td>\n",
              "      <td>-0.6136</td>\n",
              "      <td>1.1260</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.3086</td>\n",
              "      <td>0.2381</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>0.4391</td>\n",
              "      <td>-0.1009</td>\n",
              "      <td>-0.6560</td>\n",
              "      <td>-0.0124</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.4918</td>\n",
              "      <td>0.5983</td>\n",
              "      <td>1.092</td>\n",
              "      <td>0.3945</td>\n",
              "      <td>-1.6130</td>\n",
              "      <td>-0.8185</td>\n",
              "      <td>0.5568</td>\n",
              "      <td>-0.0243</td>\n",
              "      <td>2.6580</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1514</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>-0.1260</td>\n",
              "      <td>-0.3194</td>\n",
              "      <td>0.0879</td>\n",
              "      <td>0.9621</td>\n",
              "      <td>0.1206</td>\n",
              "      <td>1.2350</td>\n",
              "      <td>0.0970</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>0.2267</td>\n",
              "      <td>-0.3676</td>\n",
              "      <td>-0.2111</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>1.1340</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.7656</td>\n",
              "      <td>0.0379</td>\n",
              "      <td>0.6344</td>\n",
              "      <td>-1.2500</td>\n",
              "      <td>-0.5667</td>\n",
              "      <td>1.0420</td>\n",
              "      <td>0.5113</td>\n",
              "      <td>0.1511</td>\n",
              "      <td>-0.1806</td>\n",
              "      <td>-0.8481</td>\n",
              "      <td>0.7934</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>-0.3124</td>\n",
              "      <td>-0.8356</td>\n",
              "      <td>-0.6817</td>\n",
              "      <td>0.3114</td>\n",
              "      <td>0.2517</td>\n",
              "      <td>-0.2191</td>\n",
              "      <td>0.3795</td>\n",
              "      <td>-0.3393</td>\n",
              "      <td>1.2290</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>-0.0692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4827</th>\n",
              "      <td>id_3417d87d0</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D2</td>\n",
              "      <td>-0.0086</td>\n",
              "      <td>0.7372</td>\n",
              "      <td>0.1810</td>\n",
              "      <td>0.1930</td>\n",
              "      <td>0.5897</td>\n",
              "      <td>-0.3199</td>\n",
              "      <td>-0.5360</td>\n",
              "      <td>0.8846</td>\n",
              "      <td>0.1686</td>\n",
              "      <td>1.0280</td>\n",
              "      <td>-0.2269</td>\n",
              "      <td>-0.1723</td>\n",
              "      <td>-0.0243</td>\n",
              "      <td>-0.3090</td>\n",
              "      <td>-1.0960</td>\n",
              "      <td>-0.1916</td>\n",
              "      <td>-0.3328</td>\n",
              "      <td>0.1532</td>\n",
              "      <td>-0.3413</td>\n",
              "      <td>0.8063</td>\n",
              "      <td>-0.8786</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>-0.4091</td>\n",
              "      <td>-0.4579</td>\n",
              "      <td>2.7620</td>\n",
              "      <td>-0.1528</td>\n",
              "      <td>-0.8778</td>\n",
              "      <td>0.2447</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.402</td>\n",
              "      <td>-1.1290</td>\n",
              "      <td>-0.5179</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>0.0795</td>\n",
              "      <td>0.1999</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.4240</td>\n",
              "      <td>0.5922</td>\n",
              "      <td>0.4759</td>\n",
              "      <td>-0.1802</td>\n",
              "      <td>0.1359</td>\n",
              "      <td>0.0685</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>0.0860</td>\n",
              "      <td>-0.3379</td>\n",
              "      <td>0.8096</td>\n",
              "      <td>-0.8846</td>\n",
              "      <td>0.5055</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>-1.333</td>\n",
              "      <td>-0.1237</td>\n",
              "      <td>1.0710</td>\n",
              "      <td>1.0720</td>\n",
              "      <td>-0.1536</td>\n",
              "      <td>0.2630</td>\n",
              "      <td>0.4629</td>\n",
              "      <td>-0.6015</td>\n",
              "      <td>-0.2448</td>\n",
              "      <td>-0.4243</td>\n",
              "      <td>-0.8581</td>\n",
              "      <td>-0.5474</td>\n",
              "      <td>-0.5817</td>\n",
              "      <td>-0.3486</td>\n",
              "      <td>-0.2435</td>\n",
              "      <td>-0.4311</td>\n",
              "      <td>-0.7990</td>\n",
              "      <td>-1.0190</td>\n",
              "      <td>0.8199</td>\n",
              "      <td>0.0732</td>\n",
              "      <td>-0.2135</td>\n",
              "      <td>-0.9970</td>\n",
              "      <td>-0.5412</td>\n",
              "      <td>0.2128</td>\n",
              "      <td>-0.8640</td>\n",
              "      <td>0.5618</td>\n",
              "      <td>-0.0129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16414</th>\n",
              "      <td>id_b06db0d7e</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D2</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>0.0137</td>\n",
              "      <td>0.5486</td>\n",
              "      <td>-0.6832</td>\n",
              "      <td>0.2623</td>\n",
              "      <td>0.7661</td>\n",
              "      <td>0.0703</td>\n",
              "      <td>0.5610</td>\n",
              "      <td>-0.2339</td>\n",
              "      <td>0.5865</td>\n",
              "      <td>-0.6416</td>\n",
              "      <td>-0.5563</td>\n",
              "      <td>-0.5789</td>\n",
              "      <td>-1.0250</td>\n",
              "      <td>-0.0351</td>\n",
              "      <td>0.2711</td>\n",
              "      <td>-0.1328</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>-0.3856</td>\n",
              "      <td>-0.2448</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.3049</td>\n",
              "      <td>0.2145</td>\n",
              "      <td>0.2411</td>\n",
              "      <td>-0.6723</td>\n",
              "      <td>0.5567</td>\n",
              "      <td>1.0600</td>\n",
              "      <td>-0.8691</td>\n",
              "      <td>1.018</td>\n",
              "      <td>0.1007</td>\n",
              "      <td>-0.1240</td>\n",
              "      <td>-0.1495</td>\n",
              "      <td>-0.3582</td>\n",
              "      <td>0.0305</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>...</td>\n",
              "      <td>1.1500</td>\n",
              "      <td>-0.1783</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.4706</td>\n",
              "      <td>0.1911</td>\n",
              "      <td>-0.3037</td>\n",
              "      <td>-1.0310</td>\n",
              "      <td>0.8685</td>\n",
              "      <td>0.9621</td>\n",
              "      <td>-0.5094</td>\n",
              "      <td>0.8541</td>\n",
              "      <td>-0.0528</td>\n",
              "      <td>0.8923</td>\n",
              "      <td>1.204</td>\n",
              "      <td>-0.4681</td>\n",
              "      <td>1.7920</td>\n",
              "      <td>-0.2205</td>\n",
              "      <td>0.6291</td>\n",
              "      <td>-0.1970</td>\n",
              "      <td>0.2690</td>\n",
              "      <td>1.3110</td>\n",
              "      <td>1.0250</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>-0.3478</td>\n",
              "      <td>0.9035</td>\n",
              "      <td>0.3909</td>\n",
              "      <td>1.6770</td>\n",
              "      <td>-0.2601</td>\n",
              "      <td>0.8882</td>\n",
              "      <td>-0.2747</td>\n",
              "      <td>1.0040</td>\n",
              "      <td>-0.0146</td>\n",
              "      <td>-0.5534</td>\n",
              "      <td>0.7575</td>\n",
              "      <td>0.2705</td>\n",
              "      <td>0.3861</td>\n",
              "      <td>0.5306</td>\n",
              "      <td>0.0786</td>\n",
              "      <td>-0.1251</td>\n",
              "      <td>-0.8556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "3611   id_26c42f465  trt_cp       72      D2  ... -0.3393  1.2290  0.3202 -0.0692\n",
              "4827   id_3417d87d0  trt_cp       24      D2  ...  0.2128 -0.8640  0.5618 -0.0129\n",
              "16414  id_b06db0d7e  trt_cp       48      D2  ...  0.5306  0.0786 -0.1251 -0.8556\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ziq_dpuLvOR3",
        "outputId": "27baed1f-dd19-446a-8ac7-c9126088d599"
      },
      "source": [
        "train_targets_scored.sample(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14282</th>\n",
              "      <td>id_99bd16e2d</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12535</th>\n",
              "      <td>id_870c9962c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21481</th>\n",
              "      <td>id_e6457ea77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id  ...  wnt_inhibitor\n",
              "14282  id_99bd16e2d  ...              0\n",
              "12535  id_870c9962c  ...              0\n",
              "21481  id_e6457ea77  ...              0\n",
              "\n",
              "[3 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WngLZvEvOR3",
        "outputId": "99fddc0c-1e62-485a-815f-1978b6c9d222"
      },
      "source": [
        "data_train.cp_type.value_counts(normalize=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         0.921939\n",
              "ctl_vehicle    0.078061\n",
              "Name: cp_type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWUcbLGXvOR3",
        "outputId": "4ebdced6-a2f3-4481-ef69-879d5a69f0bf"
      },
      "source": [
        "control_group = data_train.loc[data_train.cp_type == 'ctl_vehicle']\n",
        "control_group['sig_id'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_BsuDqsC097"
      },
      "source": [
        "**Summary**:\n",
        "\n",
        "7.8357% samples in the original dataset were treated as the control perturbation => 1866 samples out of total sample pool of 23814. In the dataset, cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); Since **control perturbations have no MoAs**, samples (sig_id) with this cp_type will be droped from training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfzqKT08EWz"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "kXe-7W7GvOR3",
        "outputId": "bf484bbc-3c26-4612-d4cb-1bd815629bdc"
      },
      "source": [
        "# Define helper functions data pre-processing\n",
        "def df_pre_processing(raw_df, type='training', verbose=True):\n",
        "    # Expand features 2 non-numerical features 'cp_type', 'cp_dose' to 4 dummy \n",
        "    # Features based on categorical values \n",
        "    processed_df = pd.concat([raw_df, pd.get_dummies(raw_df['cp_dose'], prefix='cp_dose')], axis=1)\n",
        "    processed_df = pd.concat([processed_df, pd.get_dummies(raw_df['cp_type'], \\\n",
        "                                                                           prefix='cp_type')], axis=1)\n",
        "\n",
        "    # Drop the three original features\n",
        "    processed_df = processed_df.drop(['cp_type', 'cp_dose'], axis=1)\n",
        "\n",
        "    # Removed the samples with wrong cp_type -- removed 1866 samples\n",
        "    processed_df = processed_df.loc[processed_df['cp_type_trt_cp']==1].reset_index(drop=True)\n",
        "    \n",
        "    # Drop the original sig_id column\n",
        "    processed_df = processed_df.drop(columns='sig_id')\n",
        "\n",
        "    # Show shape of processed df\n",
        "    if verbose:\n",
        "        print(f\"Processed {type} dataset shape = {processed_df.shape}.\")\n",
        "        \n",
        "    return processed_df\n",
        "\n",
        "# Apply on both training and test dataset\n",
        "data_train_processed = df_pre_processing(raw_df=data_train)\n",
        "data_test_processed = df_pre_processing(raw_df=data_test)\n",
        "data_train_processed.head(3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed training dataset shape = (19759, 877).\n",
            "Processed training dataset shape = (2189, 877).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cp_time</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>g-36</th>\n",
              "      <th>g-37</th>\n",
              "      <th>g-38</th>\n",
              "      <th>...</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "      <th>cp_dose_D1</th>\n",
              "      <th>cp_dose_D2</th>\n",
              "      <th>cp_type_ctl_vehicle</th>\n",
              "      <th>cp_type_trt_cp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72</td>\n",
              "      <td>0.2894</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>-3.6110</td>\n",
              "      <td>-0.4329</td>\n",
              "      <td>-0.3965</td>\n",
              "      <td>0.1353</td>\n",
              "      <td>1.0590</td>\n",
              "      <td>-0.0249</td>\n",
              "      <td>2.4480</td>\n",
              "      <td>0.1467</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>-0.1511</td>\n",
              "      <td>1.4710</td>\n",
              "      <td>0.8354</td>\n",
              "      <td>0.0756</td>\n",
              "      <td>-0.2370</td>\n",
              "      <td>-0.6136</td>\n",
              "      <td>1.1260</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.3086</td>\n",
              "      <td>0.2381</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>0.4391</td>\n",
              "      <td>-0.1009</td>\n",
              "      <td>-0.6560</td>\n",
              "      <td>-0.0124</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.4918</td>\n",
              "      <td>0.5983</td>\n",
              "      <td>1.092</td>\n",
              "      <td>0.3945</td>\n",
              "      <td>-1.6130</td>\n",
              "      <td>-0.8185</td>\n",
              "      <td>0.5568</td>\n",
              "      <td>-0.0243</td>\n",
              "      <td>2.6580</td>\n",
              "      <td>0.8234</td>\n",
              "      <td>0.7520</td>\n",
              "      <td>-0.0345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0879</td>\n",
              "      <td>0.9621</td>\n",
              "      <td>0.1206</td>\n",
              "      <td>1.2350</td>\n",
              "      <td>0.0970</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>0.2267</td>\n",
              "      <td>-0.3676</td>\n",
              "      <td>-0.2111</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>1.1340</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.7656</td>\n",
              "      <td>0.0379</td>\n",
              "      <td>0.6344</td>\n",
              "      <td>-1.2500</td>\n",
              "      <td>-0.5667</td>\n",
              "      <td>1.0420</td>\n",
              "      <td>0.5113</td>\n",
              "      <td>0.1511</td>\n",
              "      <td>-0.1806</td>\n",
              "      <td>-0.8481</td>\n",
              "      <td>0.7934</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>-0.3124</td>\n",
              "      <td>-0.8356</td>\n",
              "      <td>-0.6817</td>\n",
              "      <td>0.3114</td>\n",
              "      <td>0.2517</td>\n",
              "      <td>-0.2191</td>\n",
              "      <td>0.3795</td>\n",
              "      <td>-0.3393</td>\n",
              "      <td>1.2290</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>-0.0692</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>-0.0086</td>\n",
              "      <td>0.7372</td>\n",
              "      <td>0.1810</td>\n",
              "      <td>0.1930</td>\n",
              "      <td>0.5897</td>\n",
              "      <td>-0.3199</td>\n",
              "      <td>-0.5360</td>\n",
              "      <td>0.8846</td>\n",
              "      <td>0.1686</td>\n",
              "      <td>1.0280</td>\n",
              "      <td>-0.2269</td>\n",
              "      <td>-0.1723</td>\n",
              "      <td>-0.0243</td>\n",
              "      <td>-0.3090</td>\n",
              "      <td>-1.0960</td>\n",
              "      <td>-0.1916</td>\n",
              "      <td>-0.3328</td>\n",
              "      <td>0.1532</td>\n",
              "      <td>-0.3413</td>\n",
              "      <td>0.8063</td>\n",
              "      <td>-0.8786</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>-0.4091</td>\n",
              "      <td>-0.4579</td>\n",
              "      <td>2.7620</td>\n",
              "      <td>-0.1528</td>\n",
              "      <td>-0.8778</td>\n",
              "      <td>0.2447</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.402</td>\n",
              "      <td>-1.1290</td>\n",
              "      <td>-0.5179</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>0.0795</td>\n",
              "      <td>0.1999</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>-3.5680</td>\n",
              "      <td>-0.1497</td>\n",
              "      <td>1.7420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1359</td>\n",
              "      <td>0.0685</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>0.0860</td>\n",
              "      <td>-0.3379</td>\n",
              "      <td>0.8096</td>\n",
              "      <td>-0.8846</td>\n",
              "      <td>0.5055</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>-1.333</td>\n",
              "      <td>-0.1237</td>\n",
              "      <td>1.0710</td>\n",
              "      <td>1.0720</td>\n",
              "      <td>-0.1536</td>\n",
              "      <td>0.2630</td>\n",
              "      <td>0.4629</td>\n",
              "      <td>-0.6015</td>\n",
              "      <td>-0.2448</td>\n",
              "      <td>-0.4243</td>\n",
              "      <td>-0.8581</td>\n",
              "      <td>-0.5474</td>\n",
              "      <td>-0.5817</td>\n",
              "      <td>-0.3486</td>\n",
              "      <td>-0.2435</td>\n",
              "      <td>-0.4311</td>\n",
              "      <td>-0.7990</td>\n",
              "      <td>-1.0190</td>\n",
              "      <td>0.8199</td>\n",
              "      <td>0.0732</td>\n",
              "      <td>-0.2135</td>\n",
              "      <td>-0.9970</td>\n",
              "      <td>-0.5412</td>\n",
              "      <td>0.2128</td>\n",
              "      <td>-0.8640</td>\n",
              "      <td>0.5618</td>\n",
              "      <td>-0.0129</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>48</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>0.0137</td>\n",
              "      <td>0.5486</td>\n",
              "      <td>-0.6832</td>\n",
              "      <td>0.2623</td>\n",
              "      <td>0.7661</td>\n",
              "      <td>0.0703</td>\n",
              "      <td>0.5610</td>\n",
              "      <td>-0.2339</td>\n",
              "      <td>0.5865</td>\n",
              "      <td>-0.6416</td>\n",
              "      <td>-0.5563</td>\n",
              "      <td>-0.5789</td>\n",
              "      <td>-1.0250</td>\n",
              "      <td>-0.0351</td>\n",
              "      <td>0.2711</td>\n",
              "      <td>-0.1328</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>-0.3856</td>\n",
              "      <td>-0.2448</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.3049</td>\n",
              "      <td>0.2145</td>\n",
              "      <td>0.2411</td>\n",
              "      <td>-0.6723</td>\n",
              "      <td>0.5567</td>\n",
              "      <td>1.0600</td>\n",
              "      <td>-0.8691</td>\n",
              "      <td>1.018</td>\n",
              "      <td>0.1007</td>\n",
              "      <td>-0.1240</td>\n",
              "      <td>-0.1495</td>\n",
              "      <td>-0.3582</td>\n",
              "      <td>0.0305</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.3884</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>-0.1698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1911</td>\n",
              "      <td>-0.3037</td>\n",
              "      <td>-1.0310</td>\n",
              "      <td>0.8685</td>\n",
              "      <td>0.9621</td>\n",
              "      <td>-0.5094</td>\n",
              "      <td>0.8541</td>\n",
              "      <td>-0.0528</td>\n",
              "      <td>0.8923</td>\n",
              "      <td>1.204</td>\n",
              "      <td>-0.4681</td>\n",
              "      <td>1.7920</td>\n",
              "      <td>-0.2205</td>\n",
              "      <td>0.6291</td>\n",
              "      <td>-0.1970</td>\n",
              "      <td>0.2690</td>\n",
              "      <td>1.3110</td>\n",
              "      <td>1.0250</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>-0.3478</td>\n",
              "      <td>0.9035</td>\n",
              "      <td>0.3909</td>\n",
              "      <td>1.6770</td>\n",
              "      <td>-0.2601</td>\n",
              "      <td>0.8882</td>\n",
              "      <td>-0.2747</td>\n",
              "      <td>1.0040</td>\n",
              "      <td>-0.0146</td>\n",
              "      <td>-0.5534</td>\n",
              "      <td>0.7575</td>\n",
              "      <td>0.2705</td>\n",
              "      <td>0.3861</td>\n",
              "      <td>0.5306</td>\n",
              "      <td>0.0786</td>\n",
              "      <td>-0.1251</td>\n",
              "      <td>-0.8556</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 877 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cp_time     g-0     g-1  ...  cp_dose_D2  cp_type_ctl_vehicle  cp_type_trt_cp\n",
              "0       72  0.2894  0.0080  ...           1                    0               1\n",
              "1       24 -0.0086  0.7372  ...           1                    0               1\n",
              "2       48 -0.1132  0.0137  ...           1                    0               1\n",
              "\n",
              "[3 rows x 877 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtDO7bEnvOR5"
      },
      "source": [
        "# Define helper functions for traget multi-binary-label classification metric\n",
        "def metric(y_true, y_pred, df_train_targets):\n",
        "    metrics = []\n",
        "    for _target in df_train_targets.columns:\n",
        "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
        "    return np.mean(metrics)\n",
        "\n",
        "def get_average_metrics_out_of_cv_folds(history_list):\n",
        "    \"\"\" Derive total number of folds and total number epochs\"\"\"\n",
        "    total_n_folds = len(history_list)\n",
        "    total_epochs = len(hist_total_list[0].history['loss'])\n",
        "    mean_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    mean_val_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    \n",
        "    # Put loss and validation loss in each fold\n",
        "    for i in range(total_n_folds):\n",
        "        mean_loss[i,:] = history_list[i].history['loss']\n",
        "        mean_val_loss[i,:] = history_list[i].history['val_loss']\n",
        "\n",
        "    # Get average loss and validation loss\n",
        "    mean_loss = np.mean(mean_loss,axis=0)\n",
        "    mean_val_loss  = np.mean(mean_val_loss ,axis=0)\n",
        "    return mean_loss, mean_val_loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UM-tyq-yMeG",
        "outputId": "df275cf5-3852-4c96-d61c-71f59b90c756"
      },
      "source": [
        "# Removed the samples with wrong cp_type on the training targets scored dataset\n",
        "train_targets_w_id = train_targets_scored.loc[data_train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "test_targets_w_id = test_targets_scored.loc[data_test['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "\n",
        "# Drop the original sig_id column\n",
        "train_targets = train_targets_w_id.drop(columns='sig_id')\n",
        "test_targets = test_targets_w_id.drop(columns='sig_id')\n",
        "\n",
        "print(f\"The processed training targets scored data shape = {train_targets.shape}.\")\n",
        "print(f\"The processed testing targets scored data shape = {test_targets.shape}.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The processed training targets scored data shape = (19759, 206).\n",
            "The processed testing targets scored data shape = (2189, 206).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAXd5Zl9alM"
      },
      "source": [
        "# Implementation of NODE based on tf.keras framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIgNmJoovOR5"
      },
      "source": [
        "@tf.function\n",
        "def sparsemoid(inputs: tf.Tensor):\n",
        "    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n",
        "\n",
        "@tf.function\n",
        "def identity(x: tf.Tensor):\n",
        "    return x\n",
        "\n",
        "class ODST(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.):\n",
        "        super(ODST, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "    \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        feature_selection_logits_init = tf.zeros_initializer()\n",
        "        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)        \n",
        "        \n",
        "        feature_thresholds_init = tf.zeros_initializer()\n",
        "        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        log_temperatures_init = tf.ones_initializer()\n",
        "        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n",
        "        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n",
        "        bin_codes = (tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2)\n",
        "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
        "        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'), \n",
        "                                          trainable=False)\n",
        "        \n",
        "        response_init = tf.ones_initializer()\n",
        "        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'), \n",
        "                                    trainable=True)\n",
        "                \n",
        "    def initialize(self, inputs):        \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        # Intialize feature_thresholds\n",
        "        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n",
        "                                                      self.threshold_init_beta)\n",
        "                         .sample([self.n_trees * self.depth]))\n",
        "        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n",
        "        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n",
        "        \n",
        "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
        "        \n",
        "        # Intialize log_temperatures\n",
        "        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n",
        "        \n",
        "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
        "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
        "        # ^--[in_features, n_trees, depth]\n",
        "\n",
        "        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n",
        "        # ^--[batch_size, n_trees, depth]\n",
        "        \n",
        "        return feature_values\n",
        "        \n",
        "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
        "        if not self.initialized:\n",
        "            self.initialize(inputs)\n",
        "            self.initialized = True\n",
        "            \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
        "\n",
        "        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n",
        "        # ^--[batch_size, n_trees, depth, 2]\n",
        "\n",
        "        bins = sparsemoid(threshold_logits)\n",
        "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
        "\n",
        "        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n",
        "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
        "\n",
        "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
        "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
        "\n",
        "        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n",
        "        # ^-- [batch_size, n_trees, units]\n",
        "        \n",
        "        return tf.reduce_sum(response, axis=1)\n",
        "        \n",
        "class NODE(tf.keras.Model):\n",
        "    def __init__(self, units: int = 1, n_layers: int = 1, output_dim = 1, \n",
        "                 dropout_rate = 0.1, link: tf.function = tf.identity, \n",
        "                 n_trees: int = 3, depth: int = 4, threshold_init_beta: float = 1., \n",
        "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None):\n",
        "        super(NODE, self).__init__()\n",
        "        self.units = units\n",
        "        self.n_layers = n_layers\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "        self.feature_column = feature_column\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "        \n",
        "        self.bn = [tf.keras.layers.BatchNormalization() for _ in range(n_layers + 1)]\n",
        "        self.dropout = [tf.keras.layers.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
        "        self.ensemble = [ODST(n_trees = n_trees,\n",
        "                              depth = depth,\n",
        "                              units = units,\n",
        "                              threshold_init_beta = threshold_init_beta) \n",
        "                         for _ in range(n_layers)]\n",
        "        \n",
        "        self.last_layer = tf.keras.layers.Dense(self.output_dim)\n",
        "        \n",
        "        self.link = link\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        X = self.feature(inputs)\n",
        "        X = self.bn[0](X, training=training)\n",
        "        X = self.dropout[0](X, training=training)\n",
        "        \n",
        "        for i, tree in enumerate(self.ensemble):\n",
        "            H = tree(X)\n",
        "            X = tf.concat([X, H], axis=1)\n",
        "            X = self.bn[i + 1](X, training=training)\n",
        "            X = self.dropout[i + 1](X, training=training)\n",
        "            \n",
        "        return self.link(self.last_layer(X))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH1V5SjuvOR5"
      },
      "source": [
        "def compile_NODE(n_layers, units, output_dim, dropout_rate, depth, n_trees, link, learning_rate):\n",
        "    \"\"\" create \n",
        "    \"\"\" \n",
        "    node = NODE(n_layers=n_layers, units=units, output_dim=output_dim, dropout_rate=dropout_rate, \n",
        "                depth=depth, n_trees=n_trees, link=tf.keras.activations.sigmoid)\n",
        "    \n",
        "    node.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "                 loss = 'binary_crossentropy')\n",
        "    return node"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwAMdlZVBELa"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bCdz1y7cSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5d3254-d1b6-4c62-c3e0-fc8868b0897d"
      },
      "source": [
        "# New version of NODE training code\n",
        "# integrated with K-Fold CV\n",
        "hist_total_list = []\n",
        "df_residual = test_targets.copy()\n",
        "df_residual.loc[:, test_targets.columns] = 0  # reset the residual matrix\n",
        "\n",
        "# Set global parameters \n",
        "N_EXPTS = 3\n",
        "N_FOLDS = 5\n",
        "top_feats = np.arange(0,877,1)\n",
        "#np.load('top_feat.npy') # top_feat.npy is avaiable in submitted data\n",
        "# If not avaiable, create the full range of index, no difference as the\n",
        "# full range here\n",
        "\n",
        "for seed in range(N_EXPTS):\n",
        "    start_time_seed = time()\n",
        "    tf.keras.backend.clear_session()  # Clear previous tf session \n",
        "    tf.random.set_seed(seed)\n",
        "    # Stratify the Multilabel K-Folds cross-validator \n",
        "    mlsk = MultilabelStratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\n",
        "    mean_loss = 0\n",
        "\n",
        "    # split training targets based on number of folds \n",
        "    for fold, (tr, val) in enumerate(mlsk.split(train_targets, train_targets)):\n",
        "        start_time_fold = time()  # track time per fold\n",
        "        x_tr, x_val = data_train_processed.values[tr][:,top_feats], data_train_processed.values[val][:,top_feats]\n",
        "        y_tr, y_val = train_targets.values[tr][:,:], train_targets.values[val][:,:]\n",
        "        \n",
        "        # NODE model instance\n",
        "        node_model = compile_NODE(n_layers=3, dropout_rate=0.1, depth=3, \n",
        "                                  n_trees=3, link=tf.keras.activations.sigmoid, \n",
        "                                  learning_rate=1e-3, units=32, output_dim=206)\n",
        "        # Adaptive alpha, monitor the learning rate and reduce the rate after 3 epochs\n",
        "        rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                verbose=0, min_delta=1e-4, mode='min')\n",
        "        # Stop training when val_loss has stopped improving\n",
        "        es = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, \n",
        "                           mode='min', restore_best_weights=True, verbose=0)\n",
        "        \n",
        "        # Start fitting NODE model\n",
        "        history = node_model.fit(x_tr, y_tr, validation_data = (x_val, y_val), \n",
        "                                 epochs=15, batch_size=128, callbacks=[rlr,es], verbose=0)\n",
        "        \n",
        "        hist_total_list.append(history)\n",
        "        hist = pd.DataFrame(history.history)\n",
        "        fold_loss = hist['val_loss'].min()\n",
        "        mean_loss += fold_loss / N_FOLDS\n",
        "\n",
        "        # Save predicted prob in residual df\n",
        "        test_predict = node_model.predict(data_test_processed.values[:,top_feats])\n",
        "        df_residual.loc[:, test_targets.columns] += test_predict / (N_EXPTS*N_FOLDS)\n",
        "        print(f'Loss @ Seed {seed}, Fold {fold} = {fold_loss}; time usage = \\\n",
        "        {str(datetime.timedelta(seconds=time()-start_time_fold))[2:7]}')\n",
        "        \n",
        "    print(f'Mean loss @ Seed {seed} = {mean_loss}; time usage = \\\n",
        "    {str(datetime.timedelta(seconds=time()-start_time_seed))[2:7]}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss @ Seed 0, Fold 0 = 0.017453033477067947; time usage =         00:40\n",
            "Loss @ Seed 0, Fold 1 = 0.017454421147704124; time usage =         00:39\n",
            "Loss @ Seed 0, Fold 2 = 0.01736772246658802; time usage =         00:40\n",
            "Loss @ Seed 0, Fold 3 = 0.01764039695262909; time usage =         00:40\n",
            "Loss @ Seed 0, Fold 4 = 0.017568593844771385; time usage =         00:40\n",
            "Mean loss @ Seed 0 = 0.017496833577752113; time usage =     03:23\n",
            "Loss @ Seed 1, Fold 0 = 0.0176056157797575; time usage =         00:40\n",
            "Loss @ Seed 1, Fold 1 = 0.017550772055983543; time usage =         00:40\n",
            "Loss @ Seed 1, Fold 2 = 0.01737077720463276; time usage =         00:40\n",
            "Loss @ Seed 1, Fold 3 = 0.01770046539604664; time usage =         00:40\n",
            "Loss @ Seed 1, Fold 4 = 0.01770775020122528; time usage =         00:39\n",
            "Mean loss @ Seed 1 = 0.017587076127529144; time usage =     03:22\n",
            "Loss @ Seed 2, Fold 0 = 0.01755533739924431; time usage =         00:40\n",
            "Loss @ Seed 2, Fold 1 = 0.017392508685588837; time usage =         00:40\n",
            "Loss @ Seed 2, Fold 2 = 0.017370210960507393; time usage =         00:39\n",
            "Loss @ Seed 2, Fold 3 = 0.01762591116130352; time usage =         00:39\n",
            "Loss @ Seed 2, Fold 4 = 0.017669357359409332; time usage =         00:39\n",
            "Mean loss @ Seed 2 = 0.017522665113210677; time usage =     03:20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Job2NgJbz2A9"
      },
      "source": [
        "# Evaluate the Model with Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X3LPrc56QMN",
        "outputId": "25bf3839-20c0-42be-8206-d70ff975bdf5"
      },
      "source": [
        "df_residual.loc[1, test_targets.columns]\n",
        "\n",
        "print('Test set Predicition Dimension: ', df_residual.shape)\n",
        "print('Test set Targets Dimension: ', test_targets.shape)\n",
        "\n",
        "print(f'NODE OOF Metric: {metric(test_targets, df_residual, test_targets)}')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set Predicition Dimension:  (2189, 206)\n",
            "Test set Targets Dimension:  (2189, 206)\n",
            "NODE OOF Metric: 0.01747743396657329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630blApY11YH"
      },
      "source": [
        "# Plot Loss and Accuracy vs. Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "M07W5E7txCFf",
        "outputId": "856c1323-27fb-4045-fbe0-a6a6c9000325"
      },
      "source": [
        "# Get average loss and validation loss out of all folds\n",
        "mean_loss, mean_val_loss = get_average_metrics_out_of_cv_folds(hist_total_list)\n",
        "loss_arr = np.vstack([mean_loss, mean_val_loss]).T\n",
        "np.save('loss_arr_NDOE_with_top_feats.npy', loss_arr)\n",
        "# Plot Loss and Accuracy vs. Epochs\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(np.arange(1,len(mean_loss)+1,1)[1:],np.array(mean_loss)[1:],'-s',lw=4,label=r'$loss_{training}$',c='k')\n",
        "plt.plot(np.arange(1,len(mean_val_loss )+1,1)[1:],np.array(mean_val_loss )[1:],'--o',lw=4,label=r'$loss_{validation}$',c='k')\n",
        "plt.ylabel('Loss',fontsize = 25,fontweight='bold')\n",
        "plt.xlabel(\"Epochs\",fontsize = 25,fontweight='bold')\n",
        "plt.legend(loc=1,frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-scha9xz79g"
      },
      "source": [
        "# Illustrate Model Summary and Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBeaXXCK1CXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffd2c29-98e0-493d-dea7-013d5774b102"
      },
      "source": [
        "print('NDOE model summary')\n",
        "node_model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDOE model summary\n",
            "Model: \"node_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_4 (Lambda)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc multiple                  3220      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc multiple                  3348      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc multiple                  3476      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc multiple                  3604      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "odst_12 (ODST)               multiple                  8079      \n",
            "_________________________________________________________________\n",
            "odst_13 (ODST)               multiple                  8367      \n",
            "_________________________________________________________________\n",
            "odst_14 (ODST)               multiple                  8655      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  185812    \n",
            "=================================================================\n",
            "Total params: 224,561\n",
            "Trainable params: 217,593\n",
            "Non-trainable params: 6,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}