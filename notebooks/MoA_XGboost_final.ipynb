{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_XGboost_final_2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuUvs3AIoGgA"
      },
      "source": [
        "# Install Necessary Pacakges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBCwhlBFQlZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c36fb4e-9215-40f3-ca63-39d304b6eb77"
      },
      "source": [
        "! pip install iterative-stratification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3j1AvG2oHUK"
      },
      "source": [
        "# Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn0JES57H41T"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le8wVeISoMRI"
      },
      "source": [
        "# Custom Function Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVY0iUZARkpW"
      },
      "source": [
        "def df_pre_processing(raw_df, type='training', verbose=True):\n",
        "    # expand features 2 non-numerical features 'cp_type', 'cp_dose' to 4 dummy \n",
        "    # features based on categorical values \n",
        "    processed_df = pd.concat([raw_df, pd.get_dummies(raw_df['cp_dose'], prefix='cp_dose')], axis=1)\n",
        "    processed_df = pd.concat([processed_df, pd.get_dummies(raw_df['cp_type'], \\\n",
        "                                                                           prefix='cp_type')], axis=1)\n",
        "\n",
        "    # drop the three original features\n",
        "    processed_df = processed_df.drop(['cp_type', 'cp_dose'], axis=1)\n",
        "\n",
        "    # removed the samples with wrong cp_type -- removed 1866 samples\n",
        "    processed_df = processed_df.loc[processed_df['cp_type_trt_cp']==1].reset_index(drop=True)\n",
        "\n",
        "    # drop the original sig_id column\n",
        "    processed_df = processed_df.drop(columns='sig_id')\n",
        "\n",
        "    # show shape of processed df\n",
        "    if verbose:\n",
        "        print(f\"Processed {type} dataset shape = {processed_df.shape}.\")\n",
        "        \n",
        "    return processed_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KniIr7oAoNvO"
      },
      "source": [
        "# Unzip and data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjb_feZlTYRM",
        "outputId": "3d92f9cb-151d-4f00-dee9-0428b4272c04"
      },
      "source": [
        "#data_path = \"./\"\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/Shareddrives/moa_data/lish-moa.zip\n",
        "data_path = '/content/'\n",
        "\n",
        "# data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA/lish-moa/\"\n",
        "\n",
        "# Load in data set\n",
        "train_features_y1 = pd.read_csv(data_path + \"train_targets_scored.csv\")\n",
        "train_features1 = pd.read_csv(data_path + \"train_features.csv\")\n",
        "\n",
        "\n",
        "train_features1, test_features1, train_features_y1 , test_features_y1 = train_test_split(train_features1, train_features_y1, test_size = 0.1, random_state = 0)\n",
        "\n",
        "# Preprocess training feature \n",
        "train_features1_processed = df_pre_processing(train_features1, type='training', verbose=True)\n",
        "test_features1_processed = df_pre_processing(test_features1, type='training', verbose=True)\n",
        "\n",
        "# Preprocess training labels\n",
        "train_features_y1_processed  = train_features_y1.loc[train_features1['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "train_features_y1_processed = train_features_y1_processed.drop(\"sig_id\", axis= 1)\n",
        "\n",
        "test_features_y1_processed  = test_features_y1.loc[test_features1['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "test_features_y1_processed = test_features_y1_processed.drop(\"sig_id\", axis= 1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/Shareddrives/moa_data/lish-moa.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_features.csv       \n",
            "  inflating: train_drug.csv          \n",
            "  inflating: train_features.csv      \n",
            "  inflating: train_targets_nonscored.csv  \n",
            "  inflating: train_targets_scored.csv  \n",
            "Processed training dataset shape = (19759, 877).\n",
            "Processed training dataset shape = (2189, 877).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF_FdU6hotKN"
      },
      "source": [
        "# Define Parameters Used for the Trainnig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0obHsc3CNFeb"
      },
      "source": [
        "X = train_features1_processed\n",
        "Y = train_features_y1_processed\n",
        "cols = Y.columns"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk6930beo3Ue"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOYt0VQuIUfX"
      },
      "source": [
        "total_loss = 0\n",
        "SEED = 42\n",
        "feature_importance_list  = []\n",
        "\n",
        "for c, column in enumerate(cols,1):\n",
        "    #print(f\"col{c} is in process\")\n",
        "    y = Y[column]\n",
        "    \n",
        "    # Split\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3,\n",
        "                                                                    random_state=SEED)    \n",
        "    \n",
        "    # Define Classifer Model\n",
        "    model = XGBClassifier(\n",
        "                         tree_method = 'gpu_hist',\n",
        "                         min_child_weight = 30,\n",
        "                         learning_rate = 0.05,\n",
        "                         colsample_bytree = 0.65,\n",
        "                         gamma = 3.705,\n",
        "                         max_depth = 10,\n",
        "                         n_estimators = 170,\n",
        "                         #subsample =  0.864, \n",
        "                         subsample =  0.9,\n",
        "                         #booster='dart',\n",
        "                         validate_parameters = True,\n",
        "                         #grow_policy = 'depthwise',\n",
        "                         predictor = 'gpu_predictor'\n",
        "                              \n",
        "                        )\n",
        "                        \n",
        "    # Train Model\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict_proba(test_features1_processed)[:,1]\n",
        "    feature_importance_list.append(model.feature_importances_)\n",
        "\n",
        "    # Loss\n",
        "    cross_entropy = -(np.array(np.log(pred+1e-5) * test_features_y1_processed[column])+ (1-test_features_y1_processed[column]) *np.log(1-pred+1e-5))\n",
        "    \n",
        "    total_loss += np.mean(cross_entropy)\n",
        "    #print(total_loss)\n",
        "    # Prediction\n",
        "    predictions = model.predict(test_features1_processed)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2f9qrNOo5YI"
      },
      "source": [
        "# Compute Mean Log Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0fNC2iUyI0I",
        "outputId": "802ef9ae-7933-4f27-dac8-39d465c1e75d"
      },
      "source": [
        "total_loss/206"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.018503261830285063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AEImzZvo-pD"
      },
      "source": [
        "# Compute Average Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOZnXh1JXFgF"
      },
      "source": [
        "feature_importance_mat = np.array(feature_importance_list)\n",
        "drop_line_ind = []\n",
        "select_line = []\n",
        "for i, feature in enumerate(feature_importance_list):\n",
        "  if sum(feature) > 0:\n",
        "    select_line.append(i)\n",
        "  else:\n",
        "    #print(f\"{i}th featrue has problem\")\n",
        "    drop_line_ind.append(i)\n",
        "\n",
        "feature_importance_arr = np.mean(feature_importance_mat[[select_line]],axis=0)\n",
        "np.save('feature_importantce_arr.npy', feature_importance_arr )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ERwi5p7pLCu"
      },
      "source": [
        "# Illustrate Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtm0ebXEbxw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cd4c57-5e35-4825-d8e3-17f88ff90378"
      },
      "source": [
        "model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=0.65, gamma=3.705,\n",
              "              learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
              "              min_child_weight=30, missing=None, n_estimators=170, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic',\n",
              "              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n",
              "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=0.9, tree_method='gpu_hist', validate_parameters=True,\n",
              "              verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}
