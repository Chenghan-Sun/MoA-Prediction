{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_NODE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uXqumWhS_M"
      },
      "source": [
        "## 2.3 Experiment of NEURAL OBLIVIOUS DECISION ENSEMBLES (NODE) on MoA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3AoDEk9UKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdeffc35-6ef8-4306-8e9b-dcf08ffdae25"
      },
      "source": [
        "# !git clone https://github.com/Qwicen/node.git\n",
        "# !pip install -r node/requirements.txt\n",
        "# !pip install qhoptim\n",
        "\n",
        "# only need this one\n",
        "! pip install iterative-stratification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M5Eop_KvORv"
      },
      "source": [
        "import sys, os\n",
        "import gc\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAvLfyoE1Rh"
      },
      "source": [
        "Run the following cell if you want to use GDrive (file stored under `/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3JosjdMzA5s",
        "outputId": "3376f400-4ed4-49f7-efe0-ef05f841be94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3SEb04vORw",
        "outputId": "2ec10f91-d81d-44c3-bc19-6db0dc42903e"
      },
      "source": [
        "# path in GDrive\n",
        "# data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA/lish-moa/\"\n",
        "\n",
        "# path\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/\"\n",
        "data_file_list = [\"train_features.csv\", \"test_features.csv\", \"train_targets_scored.csv\", \\\n",
        "                  \"train_targets_nonscored.csv\"]\n",
        "\n",
        "# load data\n",
        "data_train = pd.read_csv(data_path + data_file_list[0])\n",
        "data_test = pd.read_csv(data_path + data_file_list[1])\n",
        "train_targets_scored = pd.read_csv(data_path + data_file_list[2])\n",
        "train_targets_nonscored = pd.read_csv(data_path + data_file_list[3])\n",
        "\n",
        "# data info\n",
        "print(f'Training features file: {data_train.shape[0]} rows; {data_train.shape[1]} columns')\n",
        "print(f'Testing features file: {data_test.shape[0]} rows; {data_test.shape[1]} columns')\n",
        "print(f'Training targets scored file: {train_targets_scored.shape[0]} rows; \\\n",
        "{train_targets_scored.shape[1]} columns')\n",
        "print(f'Training targets nonscored file: {train_targets_nonscored.shape[0]} rows; \\\n",
        "{train_targets_nonscored.shape[1]} columns')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features file: 23814 rows; 876 columns\n",
            "Testing features file: 3982 rows; 876 columns\n",
            "Training targets scored file: 23814 rows; 207 columns\n",
            "Training targets nonscored file: 23814 rows; 403 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "c0QKkoK_vOR1",
        "outputId": "62266c6d-bd74-4528-c7c8-0f290e907f94"
      },
      "source": [
        "data_train.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
              "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
              "2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "sFWS1aTsvOR2",
        "outputId": "261c29f0-cccc-4ed1-e0e3-8a35ae2adc8a"
      },
      "source": [
        "data_test.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5458</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>-0.5135</td>\n",
              "      <td>0.4408</td>\n",
              "      <td>1.5500</td>\n",
              "      <td>-0.1644</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.2221</td>\n",
              "      <td>-0.3260</td>\n",
              "      <td>1.9390</td>\n",
              "      <td>-0.2305</td>\n",
              "      <td>-0.3670</td>\n",
              "      <td>1.304</td>\n",
              "      <td>1.4610</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.6816</td>\n",
              "      <td>-0.2304</td>\n",
              "      <td>-0.0635</td>\n",
              "      <td>-0.2030</td>\n",
              "      <td>-0.6821</td>\n",
              "      <td>-0.6242</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>0.2254</td>\n",
              "      <td>0.4795</td>\n",
              "      <td>0.7642</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>-0.2480</td>\n",
              "      <td>-0.1183</td>\n",
              "      <td>-0.4847</td>\n",
              "      <td>-0.0179</td>\n",
              "      <td>-0.8204</td>\n",
              "      <td>-0.5296</td>\n",
              "      <td>-1.5070</td>\n",
              "      <td>-0.0144</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1353</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.8939</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.2876</td>\n",
              "      <td>-0.3065</td>\n",
              "      <td>0.6519</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.0877</td>\n",
              "      <td>-1.0230</td>\n",
              "      <td>-0.0206</td>\n",
              "      <td>-0.4149</td>\n",
              "      <td>-0.6258</td>\n",
              "      <td>-0.2688</td>\n",
              "      <td>0.4403</td>\n",
              "      <td>-0.4900</td>\n",
              "      <td>0.2910</td>\n",
              "      <td>0.0473</td>\n",
              "      <td>-0.0914</td>\n",
              "      <td>0.3087</td>\n",
              "      <td>-0.0612</td>\n",
              "      <td>-0.9128</td>\n",
              "      <td>-0.9399</td>\n",
              "      <td>0.0173</td>\n",
              "      <td>0.0519</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>-0.5184</td>\n",
              "      <td>-0.3485</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.7978</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>-0.2067</td>\n",
              "      <td>-0.2303</td>\n",
              "      <td>-0.1193</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>-0.0502</td>\n",
              "      <td>0.1510</td>\n",
              "      <td>-0.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.1829</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>1.2080</td>\n",
              "      <td>-0.4522</td>\n",
              "      <td>-0.3652</td>\n",
              "      <td>-0.3319</td>\n",
              "      <td>-1.882</td>\n",
              "      <td>0.4022</td>\n",
              "      <td>-0.3528</td>\n",
              "      <td>0.1271</td>\n",
              "      <td>0.9303</td>\n",
              "      <td>0.3173</td>\n",
              "      <td>-1.012</td>\n",
              "      <td>-0.3213</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>-0.5389</td>\n",
              "      <td>-0.8030</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.0978</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-0.6514</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>0.5246</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.5030</td>\n",
              "      <td>-0.1500</td>\n",
              "      <td>-0.1433</td>\n",
              "      <td>2.0910</td>\n",
              "      <td>-0.6556</td>\n",
              "      <td>-0.6012</td>\n",
              "      <td>-0.4104</td>\n",
              "      <td>-0.0580</td>\n",
              "      <td>-0.3608</td>\n",
              "      <td>0.2197</td>\n",
              "      <td>-0.7101</td>\n",
              "      <td>1.3430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7458</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>-0.3644</td>\n",
              "      <td>-1.818</td>\n",
              "      <td>-0.0358</td>\n",
              "      <td>-0.7925</td>\n",
              "      <td>-0.2693</td>\n",
              "      <td>-0.0938</td>\n",
              "      <td>-0.1833</td>\n",
              "      <td>-0.7402</td>\n",
              "      <td>-1.4090</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.0460</td>\n",
              "      <td>-1.3520</td>\n",
              "      <td>-0.3445</td>\n",
              "      <td>-0.0909</td>\n",
              "      <td>-0.6337</td>\n",
              "      <td>-0.5788</td>\n",
              "      <td>-0.7885</td>\n",
              "      <td>0.0996</td>\n",
              "      <td>-1.9480</td>\n",
              "      <td>-1.2720</td>\n",
              "      <td>-0.7223</td>\n",
              "      <td>-0.5838</td>\n",
              "      <td>-1.3620</td>\n",
              "      <td>-0.7671</td>\n",
              "      <td>0.4881</td>\n",
              "      <td>0.5913</td>\n",
              "      <td>-0.4333</td>\n",
              "      <td>0.1234</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>-0.1852</td>\n",
              "      <td>-1.031</td>\n",
              "      <td>-1.3670</td>\n",
              "      <td>-0.3690</td>\n",
              "      <td>-0.5382</td>\n",
              "      <td>0.0359</td>\n",
              "      <td>-0.4764</td>\n",
              "      <td>-1.3810</td>\n",
              "      <td>-0.7300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>ctl_vehicle</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.1852</td>\n",
              "      <td>-0.1404</td>\n",
              "      <td>-0.3911</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>-1.4380</td>\n",
              "      <td>0.2455</td>\n",
              "      <td>-0.339</td>\n",
              "      <td>-0.3206</td>\n",
              "      <td>0.6944</td>\n",
              "      <td>0.5837</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>-0.6222</td>\n",
              "      <td>2.543</td>\n",
              "      <td>-0.7857</td>\n",
              "      <td>0.8163</td>\n",
              "      <td>-0.0495</td>\n",
              "      <td>0.1806</td>\n",
              "      <td>1.0290</td>\n",
              "      <td>-0.5204</td>\n",
              "      <td>-1.1070</td>\n",
              "      <td>0.7365</td>\n",
              "      <td>-0.3835</td>\n",
              "      <td>-0.5771</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>-0.2690</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.6010</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.0924</td>\n",
              "      <td>0.2785</td>\n",
              "      <td>-0.3943</td>\n",
              "      <td>-0.4602</td>\n",
              "      <td>-0.0673</td>\n",
              "      <td>-1.3420</td>\n",
              "      <td>0.3127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4369</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.6624</td>\n",
              "      <td>-0.7336</td>\n",
              "      <td>-0.5248</td>\n",
              "      <td>0.0727</td>\n",
              "      <td>0.1455</td>\n",
              "      <td>0.5364</td>\n",
              "      <td>-0.0823</td>\n",
              "      <td>0.5734</td>\n",
              "      <td>0.4876</td>\n",
              "      <td>0.7088</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>0.4689</td>\n",
              "      <td>1.0870</td>\n",
              "      <td>-0.5036</td>\n",
              "      <td>-0.3451</td>\n",
              "      <td>0.5087</td>\n",
              "      <td>1.1100</td>\n",
              "      <td>0.7886</td>\n",
              "      <td>0.2093</td>\n",
              "      <td>-0.4617</td>\n",
              "      <td>1.4870</td>\n",
              "      <td>0.1985</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>-0.5693</td>\n",
              "      <td>0.5062</td>\n",
              "      <td>-0.1925</td>\n",
              "      <td>-0.2261</td>\n",
              "      <td>0.3370</td>\n",
              "      <td>-1.384</td>\n",
              "      <td>0.8604</td>\n",
              "      <td>-1.9530</td>\n",
              "      <td>-1.0140</td>\n",
              "      <td>0.8662</td>\n",
              "      <td>1.0160</td>\n",
              "      <td>0.4924</td>\n",
              "      <td>-0.1942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id      cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_0004d9e33       trt_cp       24      D1  ...  0.0210 -0.0502  0.1510 -0.7750\n",
              "1  id_001897cda       trt_cp       72      D1  ...  0.0359 -0.4764 -1.3810 -0.7300\n",
              "2  id_002429b5b  ctl_vehicle       24      D1  ...  0.8662  1.0160  0.4924 -0.1942\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ziq_dpuLvOR3",
        "outputId": "54826dc1-9308-4dfa-afc9-70386e3a7020"
      },
      "source": [
        "train_targets_scored.sample(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10013</th>\n",
              "      <td>id_6becd24f6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9127</th>\n",
              "      <td>id_6212cde8e</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>id_042e03163</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id  ...  wnt_inhibitor\n",
              "10013  id_6becd24f6  ...              0\n",
              "9127   id_6212cde8e  ...              0\n",
              "392    id_042e03163  ...              0\n",
              "\n",
              "[3 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WngLZvEvOR3",
        "outputId": "18999a64-875f-4a28-d4ea-aaeff4633e1e"
      },
      "source": [
        "data_train.cp_type.value_counts(normalize=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         0.921643\n",
              "ctl_vehicle    0.078357\n",
              "Name: cp_type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWUcbLGXvOR3",
        "outputId": "ba0bfd8e-f2dc-4ef4-b650-87177fd4db5c"
      },
      "source": [
        "control_group = data_train.loc[data_train.cp_type == 'ctl_vehicle']\n",
        "control_group['sig_id'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_BsuDqsC097"
      },
      "source": [
        "**Summary**:\n",
        "\n",
        "7.8357% samples in the original dataset were treated as the control perturbation => 1866 samples out of total sample pool of 23814. In the dataset, cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); Since **control perturbations have no MoAs**, samples (sig_id) with this cp_type will be droped from training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfzqKT08EWz"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "kXe-7W7GvOR3",
        "outputId": "8d16b91e-c45e-4652-97dc-51ce931713b1"
      },
      "source": [
        "# define helper functions data pre-processing\n",
        "\n",
        "def df_pre_processing(raw_df, type='training', verbose=True):\n",
        "    # expand features 2 non-numerical features 'cp_type', 'cp_dose' to 4 dummy \n",
        "    # features based on categorical values \n",
        "    processed_df = pd.concat([raw_df, pd.get_dummies(raw_df['cp_dose'], prefix='cp_dose')], axis=1)\n",
        "    processed_df = pd.concat([processed_df, pd.get_dummies(raw_df['cp_type'], \\\n",
        "                                                                           prefix='cp_type')], axis=1)\n",
        "\n",
        "    # drop the three original features\n",
        "    processed_df = processed_df.drop(['cp_type', 'cp_dose'], axis=1)\n",
        "\n",
        "    # removed the samples with wrong cp_type -- removed 1866 samples\n",
        "    processed_df = processed_df.loc[processed_df['cp_type_trt_cp']==1].reset_index(drop=True)\n",
        "    \n",
        "    # drop the original sig_id column\n",
        "    processed_df = processed_df.drop(columns='sig_id')\n",
        "\n",
        "    # show shape of processed df\n",
        "    if verbose:\n",
        "        print(f\"Processed {type} dataset shape = {processed_df.shape}.\")\n",
        "        \n",
        "    return processed_df\n",
        "\n",
        "# apply on both training and test dataset\n",
        "data_train_processed = df_pre_processing(raw_df=data_train)\n",
        "data_test_processed = df_pre_processing(raw_df=data_test, type='test')\n",
        "\n",
        "data_train_processed.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed training dataset shape = (21948, 877).\n",
            "Processed test dataset shape = (3624, 877).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cp_time</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>g-36</th>\n",
              "      <th>g-37</th>\n",
              "      <th>g-38</th>\n",
              "      <th>...</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "      <th>cp_dose_D1</th>\n",
              "      <th>cp_dose_D2</th>\n",
              "      <th>cp_type_ctl_vehicle</th>\n",
              "      <th>cp_type_trt_cp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.5055</td>\n",
              "      <td>-0.5119</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>72</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>0.1656</td>\n",
              "      <td>0.5300</td>\n",
              "      <td>-0.2568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>48</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>0.1256</td>\n",
              "      <td>-0.1219</td>\n",
              "      <td>5.4470</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 877 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cp_time     g-0     g-1  ...  cp_dose_D2  cp_type_ctl_vehicle  cp_type_trt_cp\n",
              "0       24  1.0620  0.5577  ...           0                    0               1\n",
              "1       72  0.0743  0.4087  ...           0                    0               1\n",
              "2       48  0.6280  0.5817  ...           0                    0               1\n",
              "\n",
              "[3 rows x 877 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtDO7bEnvOR5"
      },
      "source": [
        "# define helper functions for traget multi-binary-label classification metric\n",
        "\n",
        "def metric(y_true, y_pred, df_train_targets):\n",
        "    metrics = []\n",
        "    for _target in df_train_targets.columns:\n",
        "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
        "    return np.mean(metrics)\n",
        "\n",
        "def get_average_metrics_out_of_cv_folds(history_list):\n",
        "    \"\"\" Derive total number of folds and total number epochs\"\"\"\n",
        "    total_n_folds = len(history_list)\n",
        "    total_epochs = len(hist_total_list[0].history['loss'])\n",
        "    mean_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    mean_val_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    \n",
        "    # Put loss and validation loss in each fold\n",
        "    for i in range(total_n_folds):\n",
        "        mean_loss[i,:] = history_list[i].history['loss']\n",
        "        mean_val_loss[i,:] = history_list[i].history['val_loss']\n",
        "\n",
        "    # Get average loss and validation loss\n",
        "    mean_loss = np.mean(mean_loss,axis=0)\n",
        "    mean_val_loss  = np.mean(mean_val_loss ,axis=0)\n",
        "    return mean_loss, mean_val_loss"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UM-tyq-yMeG",
        "outputId": "f899667c-bb30-4671-cc6a-d039a86de737"
      },
      "source": [
        "# removed the samples with wrong cp_type on the training targets scored dataset\n",
        "\n",
        "train_targets_w_id = train_targets_scored.loc[data_train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "\n",
        "# drop the original sig_id column\n",
        "train_targets = train_targets_w_id.drop(columns='sig_id')\n",
        "\n",
        "print(f\"The processed training targets scored data shape = {train_targets.shape}.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The processed training targets scored data shape = (21948, 206).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAXd5Zl9alM"
      },
      "source": [
        "### Implementation of NODE based on tf.keras framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIgNmJoovOR5"
      },
      "source": [
        "@tf.function\n",
        "def sparsemoid(inputs: tf.Tensor):\n",
        "    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n",
        "\n",
        "@tf.function\n",
        "def identity(x: tf.Tensor):\n",
        "    return x\n",
        "\n",
        "class ODST(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.):\n",
        "        super(ODST, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "    \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        feature_selection_logits_init = tf.zeros_initializer()\n",
        "        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)        \n",
        "        \n",
        "        feature_thresholds_init = tf.zeros_initializer()\n",
        "        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        log_temperatures_init = tf.ones_initializer()\n",
        "        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n",
        "        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n",
        "        bin_codes = (tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2)\n",
        "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
        "        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'), \n",
        "                                          trainable=False)\n",
        "        \n",
        "        response_init = tf.ones_initializer()\n",
        "        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'), \n",
        "                                    trainable=True)\n",
        "                \n",
        "    def initialize(self, inputs):        \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        # intialize feature_thresholds\n",
        "        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n",
        "                                                      self.threshold_init_beta)\n",
        "                         .sample([self.n_trees * self.depth]))\n",
        "        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n",
        "        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n",
        "        \n",
        "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
        "        \n",
        "        # intialize log_temperatures\n",
        "        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n",
        "        \n",
        "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
        "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
        "        # ^--[in_features, n_trees, depth]\n",
        "\n",
        "        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n",
        "        # ^--[batch_size, n_trees, depth]\n",
        "        \n",
        "        return feature_values\n",
        "        \n",
        "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
        "        if not self.initialized:\n",
        "            self.initialize(inputs)\n",
        "            self.initialized = True\n",
        "            \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
        "\n",
        "        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n",
        "        # ^--[batch_size, n_trees, depth, 2]\n",
        "\n",
        "        bins = sparsemoid(threshold_logits)\n",
        "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
        "\n",
        "        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n",
        "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
        "\n",
        "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
        "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
        "\n",
        "        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n",
        "        # ^-- [batch_size, n_trees, units]\n",
        "        \n",
        "        return tf.reduce_sum(response, axis=1)\n",
        "    \n",
        "class NODE(tf.keras.Model):\n",
        "    def __init__(self, units: int = 1, n_layers: int = 1, output_dim = 1, dropout_rate = 0.1, link: tf.function = tf.identity, n_trees: int = 3, depth: int = 4, threshold_init_beta: float = 1., feature_column: Optional[tf.keras.layers.DenseFeatures] = None):\n",
        "        super(NODE, self).__init__()\n",
        "        self.units = units\n",
        "        self.n_layers = n_layers\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "        self.feature_column = feature_column\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "        \n",
        "        self.bn = [tf.keras.layers.BatchNormalization() for _ in range(n_layers + 1)]\n",
        "        self.dropout = [tf.keras.layers.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
        "        self.ensemble = [ODST(n_trees = n_trees,\n",
        "                              depth = depth,\n",
        "                              units = units,\n",
        "                              threshold_init_beta = threshold_init_beta) \n",
        "                         for _ in range(n_layers)]\n",
        "        \n",
        "        self.last_layer = tf.keras.layers.Dense(self.output_dim)\n",
        "        \n",
        "        self.link = link\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        X = self.feature(inputs)\n",
        "        X = self.bn[0](X, training=training)\n",
        "        X = self.dropout[0](X, training=training)\n",
        "        \n",
        "        for i, tree in enumerate(self.ensemble):\n",
        "            H = tree(X)\n",
        "            X = tf.concat([X, H], axis=1)\n",
        "            X = self.bn[i + 1](X, training=training)\n",
        "            X = self.dropout[i + 1](X, training=training)\n",
        "            \n",
        "        return self.link(self.last_layer(X))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH1V5SjuvOR5"
      },
      "source": [
        "def create_NODE(n_layers, units, output_dim, dropout_rate, depth, n_trees, link, learning_rate):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    node = NODE(n_layers = n_layers, units = units, output_dim = output_dim, dropout_rate = dropout_rate, \n",
        "                depth = depth, n_trees = n_trees, link = tf.keras.activations.sigmoid)\n",
        "    \n",
        "    node.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), \n",
        "                 loss = 'binary_crossentropy')\n",
        "    return node"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwAMdlZVBELa"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaaNh7FvOR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c982fc7d-5acc-4fa0-9084-ae9ab9fc5fe2"
      },
      "source": [
        "N_STARTS = 1\n",
        "N_SPLITS = 3\n",
        "\n",
        "df_residual = train_targets.copy()\n",
        "df_residual.loc[:, train_targets.columns] = 0\n",
        "hist_total_list = []\n",
        "\n",
        "for seed in range(N_STARTS):\n",
        "    start_time_seed = time()\n",
        "    K.clear_session()\n",
        "    tf.random.set_seed(seed)\n",
        "    mean_score = 0\n",
        "    skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, random_state=42, shuffle=True)\n",
        "    \n",
        "    for n, (tr, te) in enumerate(skf.split(train_targets, train_targets)):\n",
        "        start_time_fold = time()\n",
        "        x_tr, x_val = data_train_processed.values[tr], data_train_processed.values[te]\n",
        "        y_tr, y_val = train_targets.values[tr], train_targets.values[te]\n",
        "        \n",
        "        # NODE model instance\n",
        "        model = create_NODE(n_layers=5, units=128, output_dim=206, dropout_rate=0.1, depth=6, \n",
        "                            n_trees=3, link=tf.keras.activations.sigmoid, learning_rate=1e-3)\n",
        "        # adaptive alpha, change learning rate\n",
        "        rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, \n",
        "                                min_delta=1e-4, mode = 'min')\n",
        "        # set early stop \n",
        "        es = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='min', \n",
        "                           baseline=None, restore_best_weights=True, verbose=0)\n",
        "        history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs=10, \n",
        "                            batch_size=128, callbacks=[rlr, es], verbose=0)\n",
        "        \n",
        "        hist_total_list.append(history)\n",
        "        hist = pd.DataFrame(history.history)\n",
        "        fold_score = hist['val_loss'].min()\n",
        "        mean_score += fold_score / N_SPLITS\n",
        "        val_predict = model.predict(data_train_processed.values[te])\n",
        "        \n",
        "        df_residual.loc[te, train_targets.columns] += val_predict / N_STARTS\n",
        "        print(f'[{str(datetime.timedelta(seconds = time()-start_time_fold))[2:7]}] \\\n",
        "        NODE Seed {seed}, Fold {n}:', fold_score)\n",
        "        \n",
        "    print(f'[{str(datetime.timedelta(seconds = time()-start_time_seed))[2:7]}] \\\n",
        "    NODE Seed {seed} Mean Score:', mean_score)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[01:44]         NODE Seed 0, Fold 0: 0.017650512978434563\n",
            "[01:43]         NODE Seed 0, Fold 1: 0.0175186637789011\n",
            "[01:42]         NODE Seed 0, Fold 2: 0.01755169779062271\n",
            "[05:12]     NODE Seed 0 Mean Score: 0.017573624849319458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bCdz1y7cSR"
      },
      "source": [
        "# new version \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huix0GYYvOR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2fa224-eea6-424c-82f5-e9c8114885ac"
      },
      "source": [
        "print(res.shape)\n",
        "print(train_targets.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21948, 206)\n",
            "(21948, 206)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AtdkpN58Ik"
      },
      "source": [
        "Note on expt:\n",
        "\n",
        "- performance baseline: 0.017787196522898216\n",
        "- sigmoid to softmax: \n",
        "    - no change \n",
        "- n_tree\n",
        "- n_layer\n",
        "    - change from 3 to 5: 0.017573646823844536\n",
        "- depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X3LPrc56QMN",
        "outputId": "b9e700a9-b82a-4328-cb25-4f3a3dac396d"
      },
      "source": [
        "print(f'NODE OOF Metric: {metric(train_targets, df_residual, train_targets)}')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NODE OOF Metric: 0.017573646823844536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1XI_cpY6zCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74c9242-d3c6-45f8-abba-b84b41bb74d2"
      },
      "source": [
        "# node baseline, DON'T remove \n",
        "\n",
        "print(f'NODE OOF Metric: {metric(train_targets, res, train_targets)}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NODE OOF Metric: 0.017787196522898216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630blApY11YH"
      },
      "source": [
        "### Plot Loss and Accuracy vs. Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "M07W5E7txCFf",
        "outputId": "0b4ab44a-ce75-40c2-f975-022463d9b6ec"
      },
      "source": [
        "# Get average loss and validation loss out of all folds\n",
        "mean_loss, mean_val_loss = get_average_metrics_out_of_cv_folds(hist_total_list)\n",
        "\n",
        "# Plot Loss and Accuracy vs. Epochs\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(np.arange(1,len(mean_loss)+1,1),np.array(mean_loss),'-s',lw=4,label=r'$loss_{training}$',c='r')\n",
        "plt.plot(np.arange(1,len(mean_val_loss )+1,1),np.array(mean_val_loss ),'--o',lw=4,label=r'$loss_{validation}$',c='r')\n",
        "plt.ylabel('Loss',fontsize = 25,fontweight='bold')\n",
        "plt.xlabel(\"Epochs\",fontsize = 25,fontweight='bold')\n",
        "plt.legend(loc=1,frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moQftJfz31x5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}