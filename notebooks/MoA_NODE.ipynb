{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_NODE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chenghan-Sun/MoA-Prediction/blob/master/notebooks/MoA_NODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uXqumWhS_M"
      },
      "source": [
        "## 2.3 Experiment of NEURAL OBLIVIOUS DECISION ENSEMBLES (NODE) on MoA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3AoDEk9UKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a451129-93d8-478a-f4a6-8e2067cb1362"
      },
      "source": [
        "# !git clone https://github.com/Qwicen/node.git\n",
        "# !pip install -r node/requirements.txt\n",
        "# !pip install qhoptim\n",
        "\n",
        "# only need this one\n",
        "! pip install iterative-stratification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M5Eop_KvORv"
      },
      "source": [
        "import sys, os\n",
        "import gc\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAvLfyoE1Rh"
      },
      "source": [
        "Run the following cell if you want to use GDrive (file stored under `/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3JosjdMzA5s",
        "outputId": "b131255e-3666-43b2-d1d3-f517a1748a5b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3SEb04vORw",
        "outputId": "14079ba8-56b9-441b-9573-d3ac60900467"
      },
      "source": [
        "# path in GDrive\n",
        "# data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA/lish-moa/\"\n",
        "\n",
        "# path\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/\"\n",
        "data_file_list = [\"train_features.csv\", \"test_features.csv\", \"train_targets_scored.csv\", \\\n",
        "                  \"train_targets_nonscored.csv\"]\n",
        "\n",
        "# load data\n",
        "data_train = pd.read_csv(data_path + data_file_list[0])\n",
        "data_test = pd.read_csv(data_path + data_file_list[1])\n",
        "train_targets_scored = pd.read_csv(data_path + data_file_list[2])\n",
        "train_targets_nonscored = pd.read_csv(data_path + data_file_list[3])\n",
        "\n",
        "# data info\n",
        "print(f'Training features file: {data_train.shape[0]} rows; {data_train.shape[1]} columns')\n",
        "print(f'Testing features file: {data_test.shape[0]} rows; {data_test.shape[1]} columns')\n",
        "print(f'Training targets scored file: {train_targets_scored.shape[0]} rows; \\\n",
        "{train_targets_scored.shape[1]} columns')\n",
        "print(f'Training targets nonscored file: {train_targets_nonscored.shape[0]} rows; \\\n",
        "{train_targets_nonscored.shape[1]} columns')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features file: 23814 rows; 876 columns\n",
            "Testing features file: 3982 rows; 876 columns\n",
            "Training targets scored file: 23814 rows; 207 columns\n",
            "Training targets nonscored file: 23814 rows; 403 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "c0QKkoK_vOR1",
        "outputId": "138c9fb8-02d2-423e-c55a-1edfc651f20c"
      },
      "source": [
        "data_train.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
              "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
              "2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "sFWS1aTsvOR2",
        "outputId": "e6155c99-9071-4a2e-a351-6e3c48ba9b35"
      },
      "source": [
        "data_test.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5458</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>-0.5135</td>\n",
              "      <td>0.4408</td>\n",
              "      <td>1.5500</td>\n",
              "      <td>-0.1644</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.2221</td>\n",
              "      <td>-0.3260</td>\n",
              "      <td>1.9390</td>\n",
              "      <td>-0.2305</td>\n",
              "      <td>-0.3670</td>\n",
              "      <td>1.304</td>\n",
              "      <td>1.4610</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.6816</td>\n",
              "      <td>-0.2304</td>\n",
              "      <td>-0.0635</td>\n",
              "      <td>-0.2030</td>\n",
              "      <td>-0.6821</td>\n",
              "      <td>-0.6242</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>0.2254</td>\n",
              "      <td>0.4795</td>\n",
              "      <td>0.7642</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>-0.2480</td>\n",
              "      <td>-0.1183</td>\n",
              "      <td>-0.4847</td>\n",
              "      <td>-0.0179</td>\n",
              "      <td>-0.8204</td>\n",
              "      <td>-0.5296</td>\n",
              "      <td>-1.5070</td>\n",
              "      <td>-0.0144</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1353</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.8939</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.2876</td>\n",
              "      <td>-0.3065</td>\n",
              "      <td>0.6519</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.0877</td>\n",
              "      <td>-1.0230</td>\n",
              "      <td>-0.0206</td>\n",
              "      <td>-0.4149</td>\n",
              "      <td>-0.6258</td>\n",
              "      <td>-0.2688</td>\n",
              "      <td>0.4403</td>\n",
              "      <td>-0.4900</td>\n",
              "      <td>0.2910</td>\n",
              "      <td>0.0473</td>\n",
              "      <td>-0.0914</td>\n",
              "      <td>0.3087</td>\n",
              "      <td>-0.0612</td>\n",
              "      <td>-0.9128</td>\n",
              "      <td>-0.9399</td>\n",
              "      <td>0.0173</td>\n",
              "      <td>0.0519</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>-0.5184</td>\n",
              "      <td>-0.3485</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.7978</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>-0.2067</td>\n",
              "      <td>-0.2303</td>\n",
              "      <td>-0.1193</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>-0.0502</td>\n",
              "      <td>0.1510</td>\n",
              "      <td>-0.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.1829</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>1.2080</td>\n",
              "      <td>-0.4522</td>\n",
              "      <td>-0.3652</td>\n",
              "      <td>-0.3319</td>\n",
              "      <td>-1.882</td>\n",
              "      <td>0.4022</td>\n",
              "      <td>-0.3528</td>\n",
              "      <td>0.1271</td>\n",
              "      <td>0.9303</td>\n",
              "      <td>0.3173</td>\n",
              "      <td>-1.012</td>\n",
              "      <td>-0.3213</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>-0.5389</td>\n",
              "      <td>-0.8030</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.0978</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-0.6514</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>0.5246</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.5030</td>\n",
              "      <td>-0.1500</td>\n",
              "      <td>-0.1433</td>\n",
              "      <td>2.0910</td>\n",
              "      <td>-0.6556</td>\n",
              "      <td>-0.6012</td>\n",
              "      <td>-0.4104</td>\n",
              "      <td>-0.0580</td>\n",
              "      <td>-0.3608</td>\n",
              "      <td>0.2197</td>\n",
              "      <td>-0.7101</td>\n",
              "      <td>1.3430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7458</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>-0.3644</td>\n",
              "      <td>-1.818</td>\n",
              "      <td>-0.0358</td>\n",
              "      <td>-0.7925</td>\n",
              "      <td>-0.2693</td>\n",
              "      <td>-0.0938</td>\n",
              "      <td>-0.1833</td>\n",
              "      <td>-0.7402</td>\n",
              "      <td>-1.4090</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.0460</td>\n",
              "      <td>-1.3520</td>\n",
              "      <td>-0.3445</td>\n",
              "      <td>-0.0909</td>\n",
              "      <td>-0.6337</td>\n",
              "      <td>-0.5788</td>\n",
              "      <td>-0.7885</td>\n",
              "      <td>0.0996</td>\n",
              "      <td>-1.9480</td>\n",
              "      <td>-1.2720</td>\n",
              "      <td>-0.7223</td>\n",
              "      <td>-0.5838</td>\n",
              "      <td>-1.3620</td>\n",
              "      <td>-0.7671</td>\n",
              "      <td>0.4881</td>\n",
              "      <td>0.5913</td>\n",
              "      <td>-0.4333</td>\n",
              "      <td>0.1234</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>-0.1852</td>\n",
              "      <td>-1.031</td>\n",
              "      <td>-1.3670</td>\n",
              "      <td>-0.3690</td>\n",
              "      <td>-0.5382</td>\n",
              "      <td>0.0359</td>\n",
              "      <td>-0.4764</td>\n",
              "      <td>-1.3810</td>\n",
              "      <td>-0.7300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>ctl_vehicle</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.1852</td>\n",
              "      <td>-0.1404</td>\n",
              "      <td>-0.3911</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>-1.4380</td>\n",
              "      <td>0.2455</td>\n",
              "      <td>-0.339</td>\n",
              "      <td>-0.3206</td>\n",
              "      <td>0.6944</td>\n",
              "      <td>0.5837</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>-0.6222</td>\n",
              "      <td>2.543</td>\n",
              "      <td>-0.7857</td>\n",
              "      <td>0.8163</td>\n",
              "      <td>-0.0495</td>\n",
              "      <td>0.1806</td>\n",
              "      <td>1.0290</td>\n",
              "      <td>-0.5204</td>\n",
              "      <td>-1.1070</td>\n",
              "      <td>0.7365</td>\n",
              "      <td>-0.3835</td>\n",
              "      <td>-0.5771</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>-0.2690</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.6010</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.0924</td>\n",
              "      <td>0.2785</td>\n",
              "      <td>-0.3943</td>\n",
              "      <td>-0.4602</td>\n",
              "      <td>-0.0673</td>\n",
              "      <td>-1.3420</td>\n",
              "      <td>0.3127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4369</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.6624</td>\n",
              "      <td>-0.7336</td>\n",
              "      <td>-0.5248</td>\n",
              "      <td>0.0727</td>\n",
              "      <td>0.1455</td>\n",
              "      <td>0.5364</td>\n",
              "      <td>-0.0823</td>\n",
              "      <td>0.5734</td>\n",
              "      <td>0.4876</td>\n",
              "      <td>0.7088</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>0.4689</td>\n",
              "      <td>1.0870</td>\n",
              "      <td>-0.5036</td>\n",
              "      <td>-0.3451</td>\n",
              "      <td>0.5087</td>\n",
              "      <td>1.1100</td>\n",
              "      <td>0.7886</td>\n",
              "      <td>0.2093</td>\n",
              "      <td>-0.4617</td>\n",
              "      <td>1.4870</td>\n",
              "      <td>0.1985</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>-0.5693</td>\n",
              "      <td>0.5062</td>\n",
              "      <td>-0.1925</td>\n",
              "      <td>-0.2261</td>\n",
              "      <td>0.3370</td>\n",
              "      <td>-1.384</td>\n",
              "      <td>0.8604</td>\n",
              "      <td>-1.9530</td>\n",
              "      <td>-1.0140</td>\n",
              "      <td>0.8662</td>\n",
              "      <td>1.0160</td>\n",
              "      <td>0.4924</td>\n",
              "      <td>-0.1942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id      cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_0004d9e33       trt_cp       24      D1  ...  0.0210 -0.0502  0.1510 -0.7750\n",
              "1  id_001897cda       trt_cp       72      D1  ...  0.0359 -0.4764 -1.3810 -0.7300\n",
              "2  id_002429b5b  ctl_vehicle       24      D1  ...  0.8662  1.0160  0.4924 -0.1942\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ziq_dpuLvOR3",
        "outputId": "546a8b18-0edb-40dd-fe45-bc6a29f0dbb9"
      },
      "source": [
        "train_targets_scored.sample(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12771</th>\n",
              "      <td>id_898db4ee1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22554</th>\n",
              "      <td>id_f23c45128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9646</th>\n",
              "      <td>id_67f174dac</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id  ...  wnt_inhibitor\n",
              "12771  id_898db4ee1  ...              0\n",
              "22554  id_f23c45128  ...              0\n",
              "9646   id_67f174dac  ...              0\n",
              "\n",
              "[3 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WngLZvEvOR3",
        "outputId": "509db716-46f6-454f-a428-0a2acd6a2d7e"
      },
      "source": [
        "data_train.cp_type.value_counts(normalize=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         0.921643\n",
              "ctl_vehicle    0.078357\n",
              "Name: cp_type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWUcbLGXvOR3",
        "outputId": "3f1259f1-3f85-46d6-d592-48c50f326059"
      },
      "source": [
        "control_group = data_train.loc[data_train.cp_type == 'ctl_vehicle']\n",
        "control_group['sig_id'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_BsuDqsC097"
      },
      "source": [
        "**Summary**:\n",
        "\n",
        "7.8357% samples in the original dataset were treated as the control perturbation => 1866 samples out of total sample pool of 23814. In the dataset, cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); Since **control perturbations have no MoAs**, samples (sig_id) with this cp_type will be droped from training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfzqKT08EWz"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "kXe-7W7GvOR3",
        "outputId": "870f2a2f-2d65-4034-a888-f1f6346dcb22"
      },
      "source": [
        "# define helper functions data pre-processing\n",
        "\n",
        "def df_pre_processing(raw_df, type='training', verbose=True):\n",
        "    # expand features 2 non-numerical features 'cp_type', 'cp_dose' to 4 dummy \n",
        "    # features based on categorical values \n",
        "    processed_df = pd.concat([raw_df, pd.get_dummies(raw_df['cp_dose'], prefix='cp_dose')], axis=1)\n",
        "    processed_df = pd.concat([processed_df, pd.get_dummies(raw_df['cp_type'], \\\n",
        "                                                                           prefix='cp_type')], axis=1)\n",
        "\n",
        "    # drop the three original features\n",
        "    processed_df = processed_df.drop(['cp_type', 'cp_dose'], axis=1)\n",
        "\n",
        "    # removed the samples with wrong cp_type -- removed 1866 samples\n",
        "    processed_df = processed_df.loc[processed_df['cp_type_trt_cp']==1].reset_index(drop=True)\n",
        "    \n",
        "    # drop the original sig_id column\n",
        "    processed_df = processed_df.drop(columns='sig_id')\n",
        "\n",
        "    # show shape of processed df\n",
        "    if verbose:\n",
        "        print(f\"Processed {type} dataset shape = {processed_df.shape}.\")\n",
        "        \n",
        "    return processed_df\n",
        "\n",
        "# apply on both training and test dataset\n",
        "data_train_processed = df_pre_processing(raw_df=data_train)\n",
        "data_test_processed = df_pre_processing(raw_df=data_test, type='test')\n",
        "\n",
        "data_train_processed.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed training dataset shape = (21948, 877).\n",
            "Processed test dataset shape = (3624, 877).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cp_time</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>g-36</th>\n",
              "      <th>g-37</th>\n",
              "      <th>g-38</th>\n",
              "      <th>...</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "      <th>cp_dose_D1</th>\n",
              "      <th>cp_dose_D2</th>\n",
              "      <th>cp_type_ctl_vehicle</th>\n",
              "      <th>cp_type_trt_cp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.5055</td>\n",
              "      <td>-0.5119</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>72</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>0.1656</td>\n",
              "      <td>0.5300</td>\n",
              "      <td>-0.2568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>48</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>0.1256</td>\n",
              "      <td>-0.1219</td>\n",
              "      <td>5.4470</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 877 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cp_time     g-0     g-1  ...  cp_dose_D2  cp_type_ctl_vehicle  cp_type_trt_cp\n",
              "0       24  1.0620  0.5577  ...           0                    0               1\n",
              "1       72  0.0743  0.4087  ...           0                    0               1\n",
              "2       48  0.6280  0.5817  ...           0                    0               1\n",
              "\n",
              "[3 rows x 877 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtDO7bEnvOR5"
      },
      "source": [
        "# define helper functions for traget multi-binary-label classification metric\n",
        "\n",
        "def metric(y_true, y_pred, df_train_targets):\n",
        "    metrics = []\n",
        "    for _target in df_train_targets.columns:\n",
        "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
        "    return np.mean(metrics)\n",
        "\n",
        "def get_average_metrics_out_of_cv_folds(history_list):\n",
        "    \"\"\" Derive total number of folds and total number epochs\"\"\"\n",
        "    total_n_folds = len(history_list)\n",
        "    total_epochs = len(hist_total_list[0].history['loss'])\n",
        "    mean_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    mean_val_loss = np.zeros([total_n_folds,total_epochs])\n",
        "    \n",
        "    # Put loss and validation loss in each fold\n",
        "    for i in range(total_n_folds):\n",
        "        mean_loss[i,:] = history_list[i].history['loss']\n",
        "        mean_val_loss[i,:] = history_list[i].history['val_loss']\n",
        "\n",
        "    # Get average loss and validation loss\n",
        "    mean_loss = np.mean(mean_loss,axis=0)\n",
        "    mean_val_loss  = np.mean(mean_val_loss ,axis=0)\n",
        "    return mean_loss, mean_val_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UM-tyq-yMeG",
        "outputId": "64fa38c3-bdd5-4480-9850-b13391fbccb4"
      },
      "source": [
        "# removed the samples with wrong cp_type on the training targets scored dataset\n",
        "train_targets_w_id = train_targets_scored.loc[data_train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "\n",
        "# drop the original sig_id column\n",
        "train_targets = train_targets_w_id.drop(columns='sig_id')\n",
        "\n",
        "print(f\"The processed training targets scored data shape = {train_targets.shape}.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The processed training targets scored data shape = (21948, 206).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAXd5Zl9alM"
      },
      "source": [
        "### Implementation of NODE based on tf.keras framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIgNmJoovOR5"
      },
      "source": [
        "@tf.function\n",
        "def sparsemoid(inputs: tf.Tensor):\n",
        "    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n",
        "\n",
        "@tf.function\n",
        "def identity(x: tf.Tensor):\n",
        "    return x\n",
        "\n",
        "class ODST(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.):\n",
        "        super(ODST, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "    \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        feature_selection_logits_init = tf.zeros_initializer()\n",
        "        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)        \n",
        "        \n",
        "        feature_thresholds_init = tf.zeros_initializer()\n",
        "        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        log_temperatures_init = tf.ones_initializer()\n",
        "        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n",
        "        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n",
        "        bin_codes = (tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2)\n",
        "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
        "        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'), \n",
        "                                          trainable=False)\n",
        "        \n",
        "        response_init = tf.ones_initializer()\n",
        "        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'), \n",
        "                                    trainable=True)\n",
        "                \n",
        "    def initialize(self, inputs):        \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        # intialize feature_thresholds\n",
        "        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n",
        "                                                      self.threshold_init_beta)\n",
        "                         .sample([self.n_trees * self.depth]))\n",
        "        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n",
        "        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n",
        "        \n",
        "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
        "        \n",
        "        # intialize log_temperatures\n",
        "        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n",
        "        \n",
        "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
        "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
        "        # ^--[in_features, n_trees, depth]\n",
        "\n",
        "        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n",
        "        # ^--[batch_size, n_trees, depth]\n",
        "        \n",
        "        return feature_values\n",
        "        \n",
        "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
        "        if not self.initialized:\n",
        "            self.initialize(inputs)\n",
        "            self.initialized = True\n",
        "            \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
        "\n",
        "        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n",
        "        # ^--[batch_size, n_trees, depth, 2]\n",
        "\n",
        "        bins = sparsemoid(threshold_logits)\n",
        "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
        "\n",
        "        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n",
        "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
        "\n",
        "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
        "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
        "\n",
        "        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n",
        "        # ^-- [batch_size, n_trees, units]\n",
        "        \n",
        "        return tf.reduce_sum(response, axis=1)\n",
        "        \n",
        "class NODE(tf.keras.Model):\n",
        "    def __init__(self, units: int = 1, n_layers: int = 1, output_dim = 1, dropout_rate = 0.1, link: tf.function = tf.identity, n_trees: int = 3, depth: int = 4, threshold_init_beta: float = 1., feature_column: Optional[tf.keras.layers.DenseFeatures] = None):\n",
        "        super(NODE, self).__init__()\n",
        "        self.units = units\n",
        "        self.n_layers = n_layers\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "        self.feature_column = feature_column\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "        \n",
        "        self.bn = [tf.keras.layers.BatchNormalization() for _ in range(n_layers + 1)]\n",
        "        self.dropout = [tf.keras.layers.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
        "        self.ensemble = [ODST(n_trees = n_trees,\n",
        "                              depth = depth,\n",
        "                              units = units,\n",
        "                              threshold_init_beta = threshold_init_beta) \n",
        "                         for _ in range(n_layers)]\n",
        "        \n",
        "        self.last_layer = tf.keras.layers.Dense(self.output_dim)\n",
        "        \n",
        "        self.link = link\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        X = self.feature(inputs)\n",
        "        X = self.bn[0](X, training=training)\n",
        "        X = self.dropout[0](X, training=training)\n",
        "        \n",
        "        for i, tree in enumerate(self.ensemble):\n",
        "            H = tree(X)\n",
        "            X = tf.concat([X, H], axis=1)\n",
        "            X = self.bn[i + 1](X, training=training)\n",
        "            X = self.dropout[i + 1](X, training=training)\n",
        "            \n",
        "        return self.link(self.last_layer(X))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH1V5SjuvOR5"
      },
      "source": [
        "def compile_NODE(n_layers, units, output_dim, dropout_rate, depth, n_trees, link, learning_rate):\n",
        "    \"\"\" create \n",
        "    \"\"\" \n",
        "    node = NODE(n_layers=n_layers, units=units, output_dim=output_dim, dropout_rate=dropout_rate, \n",
        "                depth=depth, n_trees=n_trees, link=tf.keras.activations.sigmoid)\n",
        "    \n",
        "    node.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "                 loss = 'binary_crossentropy')\n",
        "    return node"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwAMdlZVBELa"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bCdz1y7cSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaf481f-e3f4-423f-ca5c-466e2562bc98"
      },
      "source": [
        "# new version of NODE training code\n",
        "# integrated with K-Fold CV\n",
        "\n",
        "hist_total_list = []\n",
        "df_residual = train_targets.copy()\n",
        "df_residual.loc[:, train_targets.columns] = 0  # reset the residual matrix\n",
        "\n",
        "# set global parameters \n",
        "N_EXPTS = 3\n",
        "N_FOLDS = 5\n",
        "\n",
        "for seed in range(N_EXPTS):\n",
        "    start_time_seed = time()\n",
        "    tf.keras.backend.clear_session()  # clear previous tf session \n",
        "    tf.random.set_seed(seed)\n",
        "    # Stratify the Multilabel K-Folds cross-validator \n",
        "    mlsk = MultilabelStratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\n",
        "    mean_loss = 0\n",
        "\n",
        "    # split training targets based on number of folds \n",
        "    for fold, (tr, val) in enumerate(mlsk.split(train_targets, train_targets)):\n",
        "        start_time_fold = time()  # track time per fold\n",
        "        x_tr, x_val = data_train_processed.values[tr][:,:], data_train_processed.values[val][:,:]\n",
        "        y_tr, y_val = train_targets.values[tr][:,:], train_targets.values[val][:,:]\n",
        "        \n",
        "        # NODE model instance\n",
        "        node_model = compile_NODE(n_layers=3, dropout_rate=0.1, depth=3, \n",
        "                                  n_trees=3, link=tf.keras.activations.sigmoid, \n",
        "                                  learning_rate=1e-3, units=32, output_dim=206)\n",
        "        # adaptive alpha, monitor the learning rate and reduce the rate after 3 epochs\n",
        "        rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                verbose=0, min_delta=1e-4, mode='min')\n",
        "        # stop training when val_loss has stopped improving\n",
        "        es = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, \n",
        "                           mode='min', restore_best_weights=True, verbose=0)\n",
        "        \n",
        "        # start fitting NODE model\n",
        "        history = node_model.fit(x_tr, y_tr, validation_data = (x_val, y_val), \n",
        "                                 epochs=30, batch_size=128, callbacks=[rlr, es], verbose=0)\n",
        "        \n",
        "        hist_total_list.append(history)\n",
        "        hist = pd.DataFrame(history.history)\n",
        "        fold_loss = hist['val_loss'].min()\n",
        "        mean_loss += fold_loss / N_FOLDS\n",
        "\n",
        "        # save predicted prob in residual df\n",
        "        val_predict = node_model.predict(data_train_processed.values[val][:,:])\n",
        "        df_residual.loc[val, train_targets.columns] += val_predict / N_EXPTS\n",
        "        print(f'Loss @ Seed {seed}, Fold {fold} = {fold_loss}; time usage = \\\n",
        "        {str(datetime.timedelta(seconds=time()-start_time_fold))[2:7]}')\n",
        "        \n",
        "    print(f'Mean loss @ Seed {seed} = {mean_loss}; time usage = \\\n",
        "    {str(datetime.timedelta(seconds=time()-start_time_seed))[2:7]}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:297: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
            "[00:54]         Loss @ Seed 0, Fold 0 = 0.017161965370178223\n",
            "[00:48]         Loss @ Seed 0, Fold 1 = 0.017339009791612625\n",
            "[01:00]         Loss @ Seed 0, Fold 2 = 0.016740186139941216\n",
            "[00:51]         Loss @ Seed 0, Fold 3 = 0.017274856567382812\n",
            "[00:45]         Loss @ Seed 0, Fold 4 = 0.017443062737584114\n",
            "[04:22]     Mean loss @ Seed 0 = 0.017191816121339798\n",
            "[00:54]         Loss @ Seed 1, Fold 0 = 0.01719517819583416\n",
            "[00:45]         Loss @ Seed 1, Fold 1 = 0.01736970990896225\n",
            "[00:54]         Loss @ Seed 1, Fold 2 = 0.01676858589053154\n",
            "[00:59]         Loss @ Seed 1, Fold 3 = 0.017223818227648735\n",
            "[00:53]         Loss @ Seed 1, Fold 4 = 0.017309993505477905\n",
            "[04:30]     Mean loss @ Seed 1 = 0.017173457145690917\n",
            "[00:51]         Loss @ Seed 2, Fold 0 = 0.01720501482486725\n",
            "[00:57]         Loss @ Seed 2, Fold 1 = 0.01735789328813553\n",
            "[00:51]         Loss @ Seed 2, Fold 2 = 0.01675640419125557\n",
            "[00:59]         Loss @ Seed 2, Fold 3 = 0.017140286043286324\n",
            "[00:49]         Loss @ Seed 2, Fold 4 = 0.017251890152692795\n",
            "[04:30]     Mean loss @ Seed 2 = 0.017142297700047493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAvBLu2pPHvw",
        "outputId": "862da4f3-f682-4730-9049-c6ac8ebc56c3"
      },
      "source": [
        "df_residual.loc[1, train_targets.columns]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5-alpha_reductase_inhibitor              0.001043\n",
              "11-beta-hsd1_inhibitor                   0.000495\n",
              "acat_inhibitor                           0.001042\n",
              "acetylcholine_receptor_agonist           0.017400\n",
              "acetylcholine_receptor_antagonist        0.014149\n",
              "                                           ...   \n",
              "ubiquitin_specific_protease_inhibitor    0.000152\n",
              "vegfr_inhibitor                          0.002796\n",
              "vitamin_b                                0.001536\n",
              "vitamin_d_receptor_agonist               0.006526\n",
              "wnt_inhibitor                            0.003092\n",
              "Name: 1, Length: 206, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AtdkpN58Ik"
      },
      "source": [
        "Note on expt:\n",
        "\n",
        "- performance baseline: 0.017787196522898216\n",
        "- sigmoid to softmax: \n",
        "    - no change \n",
        "- n_tree\n",
        "- n_layer\n",
        "    - change from 3 to 5: 0.017573646823844536\n",
        "- depth\n",
        "\n",
        "- n_layer=5, n_tree=5, dropout_rate=0.05, depth=6 -> 0.017731386552524063\n",
        "\n",
        "- n_layer=3, n_tree=5, dropout_rate=0.1, depth=6 -> 0.01814203681110645\n",
        "\n",
        "- n_layer=3, n_tree=3, dropout_rate=0.1, depth=6 -> 0.017606996746624328\n",
        "\n",
        "- n_layer=3, n_tree=3, dropout_rate=0.1, depth=9 -> 0.017835741417378655"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X3LPrc56QMN",
        "outputId": "0c959afb-7492-4b0c-86b3-9ee457a317f8"
      },
      "source": [
        "print(df_residual.shape)\n",
        "print(train_targets.shape)\n",
        "\n",
        "print(f'NODE OOF Metric: {metric(train_targets, df_residual, train_targets)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21948, 206)\n",
            "(21948, 206)\n",
            "NODE OOF Metric: 0.016978945140112986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "a7RjL5vBtr0A",
        "outputId": "529d7709-46ad-40f5-e51a-ab83c5a783a2"
      },
      "source": [
        "# save important results \n",
        "\n",
        "np.save('hist_total_list.npy', hist_total_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a0b9ae45f7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save important results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hist_total_list.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_total_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 553\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function identity at 0x7f7e32610488>: it's not the same object as __main__.identity"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "630blApY11YH"
      },
      "source": [
        "### Plot Loss and Accuracy vs. Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "M07W5E7txCFf",
        "outputId": "6d8b760c-8c84-408c-d951-beb5e03ca330"
      },
      "source": [
        "# Get average loss and validation loss out of all folds\n",
        "mean_loss, mean_val_loss = get_average_metrics_out_of_cv_folds(hist_total_list)\n",
        "\n",
        "# Plot Loss and Accuracy vs. Epochs\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(np.arange(1,len(mean_loss)+1,1),np.array(mean_loss),'-s',lw=4,label=r'$loss_{training}$',c='r')\n",
        "plt.plot(np.arange(1,len(mean_val_loss )+1,1),np.array(mean_val_loss ),'--o',lw=4,label=r'$loss_{validation}$',c='r')\n",
        "plt.ylabel('Loss',fontsize = 25,fontweight='bold')\n",
        "plt.xlabel(\"Epochs\",fontsize = 25,fontweight='bold')\n",
        "plt.legend(loc=1,frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5b6467c7d715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get average loss and validation loss out of all folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_average_metrics_out_of_cv_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_total_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot Loss and Accuracy vs. Epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9a8eacd489e9>\u001b[0m in \u001b[0;36mget_average_metrics_out_of_cv_folds\u001b[0;34m(history_list)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Put loss and validation loss in each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_n_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmean_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmean_val_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 16 to array axis with dimension 17"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moQftJfz31x5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}