{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA_NODE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uXqumWhS_M"
      },
      "source": [
        "## 2.3 Experiment of NEURAL OBLIVIOUS DECISION ENSEMBLES (NODE) on MoA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3AoDEk9UKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c62829-b4f0-4f6d-9ca6-a0657ab2f4c8"
      },
      "source": [
        "# !git clone https://github.com/Qwicen/node.git\n",
        "# !pip install -r node/requirements.txt\n",
        "# !pip install qhoptim\n",
        "\n",
        "# only need this one\n",
        "! pip install iterative-stratification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.17.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M5Eop_KvORv"
      },
      "source": [
        "import sys, os\n",
        "import gc\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# from typing import Union, Optional\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAvLfyoE1Rh"
      },
      "source": [
        "Run the following cell if you want to use GDrive (file stored under `/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3JosjdMzA5s",
        "outputId": "47454a89-0ed0-49ba-b453-04f96ba62668"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3SEb04vORw",
        "outputId": "0b612fe4-562d-4452-a7e8-fa27e7d96957"
      },
      "source": [
        "# path in GDrive\n",
        "# data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA/lish-moa/\"\n",
        "\n",
        "# path\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/MoA-Prediction/moa_data/\"\n",
        "data_file_list = [\"train_features.csv\", \"test_features.csv\", \"train_targets_scored.csv\", \\\n",
        "                  \"train_targets_nonscored.csv\"]\n",
        "\n",
        "# load data\n",
        "data_train = pd.read_csv(data_path + data_file_list[0])\n",
        "data_test = pd.read_csv(data_path + data_file_list[1])\n",
        "train_targets_scored = pd.read_csv(data_path + data_file_list[2])\n",
        "train_targets_nonscored = pd.read_csv(data_path + data_file_list[3])\n",
        "\n",
        "# data info\n",
        "print(f'Training features file: {data_train.shape[0]} rows; {data_train.shape[1]} columns')\n",
        "print(f'Testing features file: {data_test.shape[0]} rows; {data_test.shape[1]} columns')\n",
        "print(f'Training targets scored file: {train_targets_scored.shape[0]} rows; \\\n",
        "{train_targets_scored.shape[1]} columns')\n",
        "print(f'Training targets nonscored file: {train_targets_nonscored.shape[0]} rows; \\\n",
        "{train_targets_nonscored.shape[1]} columns')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features file: 23814 rows; 876 columns\n",
            "Testing features file: 3982 rows; 876 columns\n",
            "Training targets scored file: 23814 rows; 207 columns\n",
            "Training targets nonscored file: 23814 rows; 403 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "c0QKkoK_vOR1",
        "outputId": "07d1f95a-ae6e-4afa-9aab-238ca03167a3"
      },
      "source": [
        "data_train.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
              "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
              "2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "sFWS1aTsvOR2",
        "outputId": "3ae87f57-a1a6-471f-f801-2fa209926b92"
      },
      "source": [
        "data_test.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5458</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>-0.5135</td>\n",
              "      <td>0.4408</td>\n",
              "      <td>1.5500</td>\n",
              "      <td>-0.1644</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.2221</td>\n",
              "      <td>-0.3260</td>\n",
              "      <td>1.9390</td>\n",
              "      <td>-0.2305</td>\n",
              "      <td>-0.3670</td>\n",
              "      <td>1.304</td>\n",
              "      <td>1.4610</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.6816</td>\n",
              "      <td>-0.2304</td>\n",
              "      <td>-0.0635</td>\n",
              "      <td>-0.2030</td>\n",
              "      <td>-0.6821</td>\n",
              "      <td>-0.6242</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>0.2254</td>\n",
              "      <td>0.4795</td>\n",
              "      <td>0.7642</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>-0.2480</td>\n",
              "      <td>-0.1183</td>\n",
              "      <td>-0.4847</td>\n",
              "      <td>-0.0179</td>\n",
              "      <td>-0.8204</td>\n",
              "      <td>-0.5296</td>\n",
              "      <td>-1.5070</td>\n",
              "      <td>-0.0144</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1353</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.8939</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.2876</td>\n",
              "      <td>-0.3065</td>\n",
              "      <td>0.6519</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.0877</td>\n",
              "      <td>-1.0230</td>\n",
              "      <td>-0.0206</td>\n",
              "      <td>-0.4149</td>\n",
              "      <td>-0.6258</td>\n",
              "      <td>-0.2688</td>\n",
              "      <td>0.4403</td>\n",
              "      <td>-0.4900</td>\n",
              "      <td>0.2910</td>\n",
              "      <td>0.0473</td>\n",
              "      <td>-0.0914</td>\n",
              "      <td>0.3087</td>\n",
              "      <td>-0.0612</td>\n",
              "      <td>-0.9128</td>\n",
              "      <td>-0.9399</td>\n",
              "      <td>0.0173</td>\n",
              "      <td>0.0519</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>-0.5184</td>\n",
              "      <td>-0.3485</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.7978</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>-0.2067</td>\n",
              "      <td>-0.2303</td>\n",
              "      <td>-0.1193</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>-0.0502</td>\n",
              "      <td>0.1510</td>\n",
              "      <td>-0.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.1829</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>1.2080</td>\n",
              "      <td>-0.4522</td>\n",
              "      <td>-0.3652</td>\n",
              "      <td>-0.3319</td>\n",
              "      <td>-1.882</td>\n",
              "      <td>0.4022</td>\n",
              "      <td>-0.3528</td>\n",
              "      <td>0.1271</td>\n",
              "      <td>0.9303</td>\n",
              "      <td>0.3173</td>\n",
              "      <td>-1.012</td>\n",
              "      <td>-0.3213</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>-0.5389</td>\n",
              "      <td>-0.8030</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.0978</td>\n",
              "      <td>-0.8156</td>\n",
              "      <td>-0.6514</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>0.5246</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.5030</td>\n",
              "      <td>-0.1500</td>\n",
              "      <td>-0.1433</td>\n",
              "      <td>2.0910</td>\n",
              "      <td>-0.6556</td>\n",
              "      <td>-0.6012</td>\n",
              "      <td>-0.4104</td>\n",
              "      <td>-0.0580</td>\n",
              "      <td>-0.3608</td>\n",
              "      <td>0.2197</td>\n",
              "      <td>-0.7101</td>\n",
              "      <td>1.3430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7458</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>-0.3644</td>\n",
              "      <td>-1.818</td>\n",
              "      <td>-0.0358</td>\n",
              "      <td>-0.7925</td>\n",
              "      <td>-0.2693</td>\n",
              "      <td>-0.0938</td>\n",
              "      <td>-0.1833</td>\n",
              "      <td>-0.7402</td>\n",
              "      <td>-1.4090</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.0460</td>\n",
              "      <td>-1.3520</td>\n",
              "      <td>-0.3445</td>\n",
              "      <td>-0.0909</td>\n",
              "      <td>-0.6337</td>\n",
              "      <td>-0.5788</td>\n",
              "      <td>-0.7885</td>\n",
              "      <td>0.0996</td>\n",
              "      <td>-1.9480</td>\n",
              "      <td>-1.2720</td>\n",
              "      <td>-0.7223</td>\n",
              "      <td>-0.5838</td>\n",
              "      <td>-1.3620</td>\n",
              "      <td>-0.7671</td>\n",
              "      <td>0.4881</td>\n",
              "      <td>0.5913</td>\n",
              "      <td>-0.4333</td>\n",
              "      <td>0.1234</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>-0.1852</td>\n",
              "      <td>-1.031</td>\n",
              "      <td>-1.3670</td>\n",
              "      <td>-0.3690</td>\n",
              "      <td>-0.5382</td>\n",
              "      <td>0.0359</td>\n",
              "      <td>-0.4764</td>\n",
              "      <td>-1.3810</td>\n",
              "      <td>-0.7300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>ctl_vehicle</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.1852</td>\n",
              "      <td>-0.1404</td>\n",
              "      <td>-0.3911</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>-1.4380</td>\n",
              "      <td>0.2455</td>\n",
              "      <td>-0.339</td>\n",
              "      <td>-0.3206</td>\n",
              "      <td>0.6944</td>\n",
              "      <td>0.5837</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>-0.6222</td>\n",
              "      <td>2.543</td>\n",
              "      <td>-0.7857</td>\n",
              "      <td>0.8163</td>\n",
              "      <td>-0.0495</td>\n",
              "      <td>0.1806</td>\n",
              "      <td>1.0290</td>\n",
              "      <td>-0.5204</td>\n",
              "      <td>-1.1070</td>\n",
              "      <td>0.7365</td>\n",
              "      <td>-0.3835</td>\n",
              "      <td>-0.5771</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>-0.2690</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.6010</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.0924</td>\n",
              "      <td>0.2785</td>\n",
              "      <td>-0.3943</td>\n",
              "      <td>-0.4602</td>\n",
              "      <td>-0.0673</td>\n",
              "      <td>-1.3420</td>\n",
              "      <td>0.3127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4369</td>\n",
              "      <td>-1.4960</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.6624</td>\n",
              "      <td>-0.7336</td>\n",
              "      <td>-0.5248</td>\n",
              "      <td>0.0727</td>\n",
              "      <td>0.1455</td>\n",
              "      <td>0.5364</td>\n",
              "      <td>-0.0823</td>\n",
              "      <td>0.5734</td>\n",
              "      <td>0.4876</td>\n",
              "      <td>0.7088</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>0.4689</td>\n",
              "      <td>1.0870</td>\n",
              "      <td>-0.5036</td>\n",
              "      <td>-0.3451</td>\n",
              "      <td>0.5087</td>\n",
              "      <td>1.1100</td>\n",
              "      <td>0.7886</td>\n",
              "      <td>0.2093</td>\n",
              "      <td>-0.4617</td>\n",
              "      <td>1.4870</td>\n",
              "      <td>0.1985</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>-0.5693</td>\n",
              "      <td>0.5062</td>\n",
              "      <td>-0.1925</td>\n",
              "      <td>-0.2261</td>\n",
              "      <td>0.3370</td>\n",
              "      <td>-1.384</td>\n",
              "      <td>0.8604</td>\n",
              "      <td>-1.9530</td>\n",
              "      <td>-1.0140</td>\n",
              "      <td>0.8662</td>\n",
              "      <td>1.0160</td>\n",
              "      <td>0.4924</td>\n",
              "      <td>-0.1942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id      cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_0004d9e33       trt_cp       24      D1  ...  0.0210 -0.0502  0.1510 -0.7750\n",
              "1  id_001897cda       trt_cp       72      D1  ...  0.0359 -0.4764 -1.3810 -0.7300\n",
              "2  id_002429b5b  ctl_vehicle       24      D1  ...  0.8662  1.0160  0.4924 -0.1942\n",
              "\n",
              "[3 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "ziq_dpuLvOR3",
        "outputId": "f1dee511-4b42-4ac0-d3e1-82ce21b1b944"
      },
      "source": [
        "train_targets_scored.sample(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17445</th>\n",
              "      <td>id_bb54d3c8b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12692</th>\n",
              "      <td>id_88c4e4164</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19107</th>\n",
              "      <td>id_cd1d0c343</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id  ...  wnt_inhibitor\n",
              "17445  id_bb54d3c8b  ...              0\n",
              "12692  id_88c4e4164  ...              0\n",
              "19107  id_cd1d0c343  ...              0\n",
              "\n",
              "[3 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WngLZvEvOR3",
        "outputId": "01d77abf-039b-4086-95eb-ce08c2711314"
      },
      "source": [
        "data_train.cp_type.value_counts(normalize=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         0.921643\n",
              "ctl_vehicle    0.078357\n",
              "Name: cp_type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWUcbLGXvOR3",
        "outputId": "3d05157e-1b85-4534-8b55-9ac5d98eb5ec"
      },
      "source": [
        "control_group = data_train.loc[data_train.cp_type == 'ctl_vehicle']\n",
        "control_group['sig_id'].count()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_BsuDqsC097"
      },
      "source": [
        "**Summary**:\n",
        "\n",
        "7.8357% samples in the original dataset were treated as the control perturbation => 1866 samples out of total sample pool of 23814. In the dataset, cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); Since **control perturbations have no MoAs**, samples (sig_id) with this cp_type will be droped from training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfzqKT08EWz"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "kXe-7W7GvOR3",
        "outputId": "7c2f927b-7dc4-467f-9c47-26e000721ebb"
      },
      "source": [
        "# define helper functions data pre-processing\n",
        "\n",
        "def df_pre_processing(raw_df, type='training', verbose=True):\n",
        "    # expand features 2 non-numerical features 'cp_type', 'cp_dose' to 4 dummy \n",
        "    # features based on categorical values \n",
        "    processed_df = pd.concat([raw_df, pd.get_dummies(raw_df['cp_dose'], prefix='cp_dose')], axis=1)\n",
        "    processed_df = pd.concat([processed_df, pd.get_dummies(raw_df['cp_type'], \\\n",
        "                                                                           prefix='cp_type')], axis=1)\n",
        "\n",
        "    # drop the three original features\n",
        "    processed_df = processed_df.drop(['cp_type', 'cp_dose'], axis=1)\n",
        "\n",
        "    # removed the samples with wrong cp_type -- removed 1866 samples\n",
        "    processed_df = processed_df.loc[processed_df['cp_type_trt_cp']==1].reset_index(drop=True)\n",
        "\n",
        "    # drop the original sig_id column\n",
        "    processed_df = processed_df.drop(columns='sig_id')\n",
        "\n",
        "    # show shape of processed df\n",
        "    if verbose:\n",
        "        print(f\"Processed {type} dataset shape = {processed_df.shape}.\")\n",
        "        \n",
        "    return processed_df\n",
        "\n",
        "# apply on both training and test dataset\n",
        "data_train_processed = df_pre_processing(raw_df=data_train)\n",
        "data_test_processed = df_pre_processing(raw_df=data_test, type='test')\n",
        "\n",
        "data_train_processed.head(3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed training dataset shape = (21948, 877).\n",
            "Processed test dataset shape = (3624, 877).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cp_time</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>g-36</th>\n",
              "      <th>g-37</th>\n",
              "      <th>g-38</th>\n",
              "      <th>...</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "      <th>cp_dose_D1</th>\n",
              "      <th>cp_dose_D2</th>\n",
              "      <th>cp_type_ctl_vehicle</th>\n",
              "      <th>cp_type_trt_cp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.5055</td>\n",
              "      <td>-0.5119</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>72</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>0.1656</td>\n",
              "      <td>0.5300</td>\n",
              "      <td>-0.2568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>48</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>0.1256</td>\n",
              "      <td>-0.1219</td>\n",
              "      <td>5.4470</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 877 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cp_time     g-0     g-1  ...  cp_dose_D2  cp_type_ctl_vehicle  cp_type_trt_cp\n",
              "0       24  1.0620  0.5577  ...           0                    0               1\n",
              "1       72  0.0743  0.4087  ...           0                    0               1\n",
              "2       48  0.6280  0.5817  ...           0                    0               1\n",
              "\n",
              "[3 rows x 877 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtDO7bEnvOR5"
      },
      "source": [
        "# define helper functions for traget multi-binary-label classification metric\n",
        "\n",
        "def metric(y_true, y_pred):\n",
        "    metrics = []\n",
        "    for _target in train_targets_scored.columns:\n",
        "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
        "    return np.mean(metrics)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UM-tyq-yMeG",
        "outputId": "0c8389e3-685b-461a-a62c-577224bc253c"
      },
      "source": [
        "# removed the samples with wrong cp_type on the training targets scored dataset\n",
        "\n",
        "train_targets = train_targets_scored.loc[data_train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
        "print(f\"The processed training targets scored data shape = {train_targets.shape}.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The processed training targets scored data shape = (21948, 207).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAXd5Zl9alM"
      },
      "source": [
        "### Implementation of NODE based on tf.keras framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vGnNY1UvOR5",
        "outputId": "f805ed69-d7a7-4869-c26d-03b958ebe130"
      },
      "source": [
        "top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
        "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
        "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
        "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
        "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
        "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
        "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
        "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
        "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
        "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
        "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
        "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
        "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
        "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
        "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
        "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
        "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
        "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
        "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
        "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
        "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
        "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
        "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
        "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
        "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
        "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
        "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
        "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
        "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
        "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
        "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
        "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
        "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
        "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
        "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
        "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
        "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
        "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
        "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
        "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
        "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
        "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
        "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
        "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
        "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
        "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
        "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
        "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
        "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
        "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
        "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
        "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
        "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
        "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
        "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
        "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
        "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
        "       870, 871, 872, 873, 874]\n",
        "\n",
        "print(len(top_feats))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIgNmJoovOR5"
      },
      "source": [
        "@tf.function\n",
        "def sparsemoid(inputs: tf.Tensor):\n",
        "    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n",
        "\n",
        "@tf.function\n",
        "def identity(x: tf.Tensor):\n",
        "    return x\n",
        "\n",
        "class ODST(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1.):\n",
        "        super(ODST, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "    \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        feature_selection_logits_init = tf.zeros_initializer()\n",
        "        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)        \n",
        "        \n",
        "        feature_thresholds_init = tf.zeros_initializer()\n",
        "        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        log_temperatures_init = tf.ones_initializer()\n",
        "        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n",
        "                                 trainable=True)\n",
        "        \n",
        "        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n",
        "        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n",
        "        bin_codes = (tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2)\n",
        "        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n",
        "        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'), \n",
        "                                          trainable=False)\n",
        "        \n",
        "        response_init = tf.ones_initializer()\n",
        "        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'), \n",
        "                                    trainable=True)\n",
        "                \n",
        "    def initialize(self, inputs):        \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        # intialize feature_thresholds\n",
        "        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n",
        "                                                      self.threshold_init_beta)\n",
        "                         .sample([self.n_trees * self.depth]))\n",
        "        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n",
        "        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n",
        "        \n",
        "        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n",
        "        \n",
        "        \n",
        "        # intialize log_temperatures\n",
        "        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n",
        "        \n",
        "        \n",
        "        \n",
        "    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n",
        "        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n",
        "        # ^--[in_features, n_trees, depth]\n",
        "\n",
        "        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n",
        "        # ^--[batch_size, n_trees, depth]\n",
        "        \n",
        "        return feature_values\n",
        "        \n",
        "    def call(self, inputs: tf.Tensor, training: bool = None):\n",
        "        if not self.initialized:\n",
        "            self.initialize(inputs)\n",
        "            self.initialized = True\n",
        "            \n",
        "        feature_values = self.feature_values(inputs)\n",
        "        \n",
        "        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n",
        "\n",
        "        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n",
        "        # ^--[batch_size, n_trees, depth, 2]\n",
        "\n",
        "        bins = sparsemoid(threshold_logits)\n",
        "        # ^--[batch_size, n_trees, depth, 2], approximately binary\n",
        "\n",
        "        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n",
        "        # ^--[batch_size, n_trees, depth, 2 ** depth]\n",
        "\n",
        "        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n",
        "        # ^-- [batch_size, n_trees, 2 ** depth]\n",
        "\n",
        "        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n",
        "        # ^-- [batch_size, n_trees, units]\n",
        "        \n",
        "        return tf.reduce_sum(response, axis=1)\n",
        "    \n",
        "class NODE(tf.keras.Model):\n",
        "    def __init__(self, units: int = 1, n_layers: int = 1, output_dim = 1, dropout_rate = 0.1, link: tf.function = tf.identity, n_trees: int = 3, depth: int = 4, threshold_init_beta: float = 1., feature_column: Optional[tf.keras.layers.DenseFeatures] = None):\n",
        "        super(NODE, self).__init__()\n",
        "        self.units = units\n",
        "        self.n_layers = n_layers\n",
        "        self.n_trees = n_trees\n",
        "        self.depth = depth\n",
        "        self.units = units\n",
        "        self.threshold_init_beta = threshold_init_beta\n",
        "        self.feature_column = feature_column\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "        \n",
        "        self.bn = [tf.keras.layers.BatchNormalization() for _ in range(n_layers + 1)]\n",
        "        self.dropout = [tf.keras.layers.Dropout(self.dropout_rate) for _ in range(n_layers + 1)]\n",
        "        self.ensemble = [ODST(n_trees = n_trees,\n",
        "                              depth = depth,\n",
        "                              units = units,\n",
        "                              threshold_init_beta = threshold_init_beta) \n",
        "                         for _ in range(n_layers)]\n",
        "        \n",
        "        self.last_layer = tf.keras.layers.Dense(self.output_dim)\n",
        "        \n",
        "        self.link = link\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        X = self.feature(inputs)\n",
        "        X = self.bn[0](X, training=training)\n",
        "        X = self.dropout[0](X, training=training)\n",
        "        \n",
        "        for i, tree in enumerate(self.ensemble):\n",
        "            H = tree(X)\n",
        "            X = tf.concat([X, H], axis=1)\n",
        "            X = self.bn[i + 1](X, training=training)\n",
        "            X = self.dropout[i + 1](X, training=training)\n",
        "            \n",
        "        return self.link(self.last_layer(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH1V5SjuvOR5"
      },
      "source": [
        "def create_NODE(n_layers, units, output_dim, dropout_rate, depth, n_trees, link, learning_rate):\n",
        "    \n",
        "    node = NODE(n_layers = n_layers, units = units, output_dim = output_dim, dropout_rate = dropout_rate, \n",
        "                depth = depth, n_trees = n_trees, link = tf.keras.activations.sigmoid)\n",
        "    \n",
        "    node.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), \n",
        "                 loss = 'binary_crossentropy')\n",
        "    \n",
        "    return node"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwAMdlZVBELa"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaaNh7FvOR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afe9471-708f-4396-d2c8-1925e1714369"
      },
      "source": [
        "N_STARTS = 7\n",
        "N_SPLITS = 7\n",
        "\n",
        "res = train_targets.copy()\n",
        "\n",
        "ss.loc[:, train_targets.columns] = 0\n",
        "res.loc[:, train_targets.columns] = 0\n",
        "\n",
        "for seed in range(N_STARTS):\n",
        "    start_time_seed = time()\n",
        "    K.clear_session()\n",
        "    tf.random.set_seed(seed)\n",
        "    mean_score = 0\n",
        "    skf = MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = 42, shuffle = True)\n",
        "    for n, (tr, te) in enumerate(skf.split(train_targets, train_targets)):\n",
        "        \n",
        "        start_time_fold = time()\n",
        "        x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n",
        "        y_tr, y_val = train_targets.values[tr], train_targets.values[te]\n",
        "        x_tt = test.values[:, top_feats]\n",
        "            \n",
        "        model = create_NODE(n_layers = 3, units = 128, output_dim = 206, dropout_rate = 0.1, depth = 6, \n",
        "                            n_trees = 3, link = tf.keras.activations.sigmoid, learning_rate = 1e-3)\n",
        "        rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, \n",
        "                                verbose = 0, min_delta = 1e-4, mode = 'min')\n",
        "        es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n",
        "                           baseline = None, restore_best_weights = True, verbose = 0)\n",
        "        history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = 100, \n",
        "                            batch_size = 128, callbacks = [rlr, es], verbose = 0)\n",
        "        \n",
        "        hist = pd.DataFrame(history.history)\n",
        "        fold_score = hist['val_loss'].min()\n",
        "        mean_score += fold_score / N_SPLITS\n",
        "        test_predict = model.predict(test.values[:, top_feats])\n",
        "        val_predict = model.predict(train.values[te][:, top_feats])\n",
        "        \n",
        "        ss.loc[:, train_targets.columns] += test_predict / (N_SPLITS * N_STARTS)\n",
        "        res.loc[te, train_targets.columns] += val_predict / N_STARTS\n",
        "        print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[2:7]}] NODE Seed {seed}, Fold {n}:', fold_score)\n",
        "        \n",
        "    print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[2:7]}] NODE Seed {seed} Mean Score:', mean_score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:297: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
            "[01:12] NODE Seed 0, Fold 0: 0.01701156236231327\n",
            "[01:11] NODE Seed 0, Fold 1: 0.01678956300020218\n",
            "[01:08] NODE Seed 0, Fold 2: 0.01710021309554577\n",
            "[01:09] NODE Seed 0, Fold 3: 0.016864197328686714\n",
            "[01:01] NODE Seed 0, Fold 4: 0.017374027520418167\n",
            "[01:04] NODE Seed 0, Fold 5: 0.017022283747792244\n",
            "[01:06] NODE Seed 0, Fold 6: 0.017274726182222366\n",
            "[07:56] NODE Seed 0 Mean Score: 0.017062367605311532\n",
            "[01:19] NODE Seed 1, Fold 0: 0.017079735174775124\n",
            "[01:01] NODE Seed 1, Fold 1: 0.017050238326191902\n",
            "[00:59] NODE Seed 1, Fold 2: 0.01717204973101616\n",
            "[01:04] NODE Seed 1, Fold 3: 0.01679457351565361\n",
            "[01:17] NODE Seed 1, Fold 4: 0.017180290073156357\n",
            "[00:59] NODE Seed 1, Fold 5: 0.016795646399259567\n",
            "[01:06] NODE Seed 1, Fold 6: 0.017311105504631996\n",
            "[07:49] NODE Seed 1 Mean Score: 0.017054805532097816\n",
            "[01:12] NODE Seed 2, Fold 0: 0.017003420740365982\n",
            "[01:09] NODE Seed 2, Fold 1: 0.01688455417752266\n",
            "[01:12] NODE Seed 2, Fold 2: 0.0171004980802536\n",
            "[01:14] NODE Seed 2, Fold 3: 0.016936004161834717\n",
            "[01:09] NODE Seed 2, Fold 4: 0.017257507890462875\n",
            "[01:14] NODE Seed 2, Fold 5: 0.016672709956765175\n",
            "[01:09] NODE Seed 2, Fold 6: 0.017090795561671257\n",
            "[08:25] NODE Seed 2 Mean Score: 0.016992212938410894\n",
            "[01:09] NODE Seed 3, Fold 0: 0.017067119479179382\n",
            "[01:07] NODE Seed 3, Fold 1: 0.0168497022241354\n",
            "[01:04] NODE Seed 3, Fold 2: 0.017368009313941002\n",
            "[00:56] NODE Seed 3, Fold 3: 0.017063094303011894\n",
            "[01:10] NODE Seed 3, Fold 4: 0.017281660810112953\n",
            "[01:04] NODE Seed 3, Fold 5: 0.016922907903790474\n",
            "[01:15] NODE Seed 3, Fold 6: 0.017185740172863007\n",
            "[07:50] NODE Seed 3 Mean Score: 0.0171054620295763\n",
            "[01:26] NODE Seed 4, Fold 0: 0.01699402928352356\n",
            "[01:07] NODE Seed 4, Fold 1: 0.01699438877403736\n",
            "[01:10] NODE Seed 4, Fold 2: 0.01704339310526848\n",
            "[01:05] NODE Seed 4, Fold 3: 0.01685880497097969\n",
            "[01:02] NODE Seed 4, Fold 4: 0.0172876063734293\n",
            "[01:05] NODE Seed 4, Fold 5: 0.016642002388834953\n",
            "[01:15] NODE Seed 4, Fold 6: 0.017215432599186897\n",
            "[08:15] NODE Seed 4 Mean Score: 0.017005093927894323\n",
            "[01:18] NODE Seed 5, Fold 0: 0.016979295760393143\n",
            "[01:18] NODE Seed 5, Fold 1: 0.016798697412014008\n",
            "[01:18] NODE Seed 5, Fold 2: 0.017240220680832863\n",
            "[01:04] NODE Seed 5, Fold 3: 0.016754962503910065\n",
            "[01:18] NODE Seed 5, Fold 4: 0.017253452911973\n",
            "[01:07] NODE Seed 5, Fold 5: 0.016714785248041153\n",
            "[01:02] NODE Seed 5, Fold 6: 0.017199456691741943\n",
            "[08:30] NODE Seed 5 Mean Score: 0.016991553029843738\n",
            "[01:18] NODE Seed 6, Fold 0: 0.016986768692731857\n",
            "[01:01] NODE Seed 6, Fold 1: 0.01697194017469883\n",
            "[01:16] NODE Seed 6, Fold 2: 0.017068922519683838\n",
            "[01:01] NODE Seed 6, Fold 3: 0.017020832747220993\n",
            "[01:21] NODE Seed 6, Fold 4: 0.017156662419438362\n",
            "[01:21] NODE Seed 6, Fold 5: 0.01655539870262146\n",
            "[01:10] NODE Seed 6, Fold 6: 0.01718193292617798\n",
            "[08:33] NODE Seed 6 Mean Score: 0.016991779740367616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huix0GYYvOR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23edac6f-a0ad-47b5-8f58-88c0680acea0"
      },
      "source": [
        "print(f'NODE OOF Metric: {metric(train_targets, res)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NODE OOF Metric: 0.016735736165448413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1XI_cpY6zCn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}